{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c87623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import contractions\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b596713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(data: pd.Series):\n",
    "    return data.str.lower()\n",
    "\n",
    "def remove_accented_characters(data: pd.Series):\n",
    "    return data.apply(lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\"))\n",
    "\n",
    "def remove_html_encodings(data: pd.Series):\n",
    "    return data.str.replace(r\"\\d+;\", \" \", regex=True)\n",
    "\n",
    "def remove_html_tags(data: pd.Series):\n",
    "    return data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)\n",
    "\n",
    "def remove_url(data: pd.Series):\n",
    "    return data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)\n",
    "\n",
    "def remove_non_alpha_characters(data: pd.Series):\n",
    "    return data.str.replace(r\"_+|\\|[^a-zA-Z0-9\\s]\", \" \", regex=True)\n",
    "\n",
    "def remove_extra_spaces(data: pd.Series):\n",
    "    return data.str.replace(r\"^\\s*|\\s\\s*\", \" \", regex=True)\n",
    "\n",
    "def fix_contractions(data: pd.Series):\n",
    "    def contraction_fixer(txt: str):\n",
    "        return \" \".join([contractions.fix(word) for word in txt.split()])\n",
    "\n",
    "    return data.apply(contraction_fixer)\n",
    "\n",
    "def remove_special_words(data: pd.Series):\n",
    "    return data.str.replace(r\"\\-[^a-zA-Z]{3}\\-\", \" \", regex=True)\n",
    "\n",
    "def get_train_test_split(model_data, x_columns, y_column, stratify_column):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        model_data[x_columns],\n",
    "        model_data[y_column],\n",
    "        train_size=0.8, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        shuffle=True,\n",
    "        stratify=model_data[stratify_column])\n",
    "    \n",
    "    print(\"Train: \",X_train.shape, Y_train.shape,\n",
    "      \"Test: \",(X_test.shape, Y_test.shape))\n",
    "    print(type(X_train))\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "cls = \"[CLS]\"\n",
    "sep = \"[SEP]\"\n",
    "pad = \"[PAD]\"\n",
    "space = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8e5c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_10004</th>\n",
       "      <td>A few months. How long have you been involved ...</td>\n",
       "      <td>And of those few months, how long have you bee...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_10009</th>\n",
       "      <td>Ah-da-da-da-da! What the hell?! Excuse me? Tha...</td>\n",
       "      <td>Let the dead man talk. So, why do you think that?</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_1001</th>\n",
       "      <td>It's smashed beyond repair. What are you gonna...</td>\n",
       "      <td>What else? Sell it on eBay as \"slightly used.\"</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_1003</th>\n",
       "      <td>I'm gonna go back and try talking to her again...</td>\n",
       "      <td>Good idea, sit with her. Hold her, comfort her...</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_10190</th>\n",
       "      <td>Sure. What's up? Leonard, I could use your ass...</td>\n",
       "      <td>Well, now that I've given up string theory, I'...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_S06E02_398</th>\n",
       "      <td>I mean, he really, really likes Pied Piper. He...</td>\n",
       "      <td>Look, we cannot take blood money.</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_S06E03_366</th>\n",
       "      <td>Right. Yeah. -we could just buy Hooli. -(laugh...</td>\n",
       "      <td>The-the same way we can buy America and everyt...</td>\n",
       "      <td>22</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_S06E05_355</th>\n",
       "      <td>I was just curious to know, like, what's it li...</td>\n",
       "      <td>Well, maybe some time when you're working on s...</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_S06E06_143</th>\n",
       "      <td>-Were you gonna tell me about this? -No. You g...</td>\n",
       "      <td>I thought that was the company policy-these days.</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_S06E07_272</th>\n",
       "      <td>I realized something. But a few hours later wh...</td>\n",
       "      <td>That you're an alcoholic?</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1202 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        context  \\\n",
       "scene                                                             \n",
       "1_10004       A few months. How long have you been involved ...   \n",
       "1_10009       Ah-da-da-da-da! What the hell?! Excuse me? Tha...   \n",
       "1_1001        It's smashed beyond repair. What are you gonna...   \n",
       "1_1003        I'm gonna go back and try talking to her again...   \n",
       "1_10190       Sure. What's up? Leonard, I could use your ass...   \n",
       "...                                                         ...   \n",
       "3_S06E02_398  I mean, he really, really likes Pied Piper. He...   \n",
       "3_S06E03_366  Right. Yeah. -we could just buy Hooli. -(laugh...   \n",
       "3_S06E05_355  I was just curious to know, like, what's it li...   \n",
       "3_S06E06_143  -Were you gonna tell me about this? -No. You g...   \n",
       "3_S06E07_272  I realized something. But a few hours later wh...   \n",
       "\n",
       "                                                         target  speaker  \\\n",
       "scene                                                                      \n",
       "1_10004       And of those few months, how long have you bee...       25   \n",
       "1_10009       Let the dead man talk. So, why do you think that?       15   \n",
       "1_1001           What else? Sell it on eBay as \"slightly used.\"       21   \n",
       "1_1003        Good idea, sit with her. Hold her, comfort her...        7   \n",
       "1_10190       Well, now that I've given up string theory, I'...       25   \n",
       "...                                                         ...      ...   \n",
       "3_S06E02_398                  Look, we cannot take blood money.       14   \n",
       "3_S06E03_366  The-the same way we can buy America and everyt...       22   \n",
       "3_S06E05_355  Well, maybe some time when you're working on s...       14   \n",
       "3_S06E06_143  I thought that was the company policy-these days.        6   \n",
       "3_S06E07_272                          That you're an alcoholic?        3   \n",
       "\n",
       "              sarcasm  \n",
       "scene                  \n",
       "1_10004           0.0  \n",
       "1_10009           0.0  \n",
       "1_1001            0.0  \n",
       "1_1003            1.0  \n",
       "1_10190           0.0  \n",
       "...               ...  \n",
       "3_S06E02_398      0.0  \n",
       "3_S06E03_366      1.0  \n",
       "3_S06E05_355      1.0  \n",
       "3_S06E06_143      1.0  \n",
       "3_S06E07_272      1.0  \n",
       "\n",
       "[1202 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"text_data_compiled.csv\")\n",
    "data = data.drop(columns=['key', 'show', 'sarcasm_type'])\n",
    "data.set_index('scene', inplace = True)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "data['speaker'] = le.fit_transform(data['speaker'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574b4b0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_cleaning_pipeline = [\n",
    "        to_lower,\n",
    "        remove_special_words,\n",
    "        remove_accented_characters,\n",
    "        remove_html_encodings,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        fix_contractions,\n",
    "        remove_non_alpha_characters,\n",
    "        remove_extra_spaces]\n",
    "\n",
    "inputs = [\"target\", \"context\"]\n",
    "\n",
    "def clean_data(data):\n",
    "    data_copy = data.copy()\n",
    "    for col in inputs:\n",
    "        temp_data = data_copy[col].copy()\n",
    "        for func in data_cleaning_pipeline:\n",
    "            temp_data = func(temp_data)\n",
    "        data_copy[col] = temp_data.copy()\n",
    "    return data_copy\n",
    "\n",
    "cleaned_data = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90606ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_10004</th>\n",
       "      <td>a few months. how long have you been involved...</td>\n",
       "      <td>and of those few months, how long have you be...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_10009</th>\n",
       "      <td>ah-da-da-da-da! what the hell?! excuse me? th...</td>\n",
       "      <td>let the dead man talk. so, why do you think t...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_1001</th>\n",
       "      <td>it is smashed beyond repair. what are you goi...</td>\n",
       "      <td>what else? sell it on ebay as \"slightly used.\"</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_1003</th>\n",
       "      <td>i am going to go back and try talking to her ...</td>\n",
       "      <td>good idea, sit with her. hold her, comfort he...</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_10190</th>\n",
       "      <td>sure. what is up? leonard, i could use your a...</td>\n",
       "      <td>well, now that i have given up string theory,...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   context  \\\n",
       "scene                                                        \n",
       "1_10004   a few months. how long have you been involved...   \n",
       "1_10009   ah-da-da-da-da! what the hell?! excuse me? th...   \n",
       "1_1001    it is smashed beyond repair. what are you goi...   \n",
       "1_1003    i am going to go back and try talking to her ...   \n",
       "1_10190   sure. what is up? leonard, i could use your a...   \n",
       "\n",
       "                                                    target  speaker  sarcasm  \n",
       "scene                                                                         \n",
       "1_10004   and of those few months, how long have you be...       25      0.0  \n",
       "1_10009   let the dead man talk. so, why do you think t...       15      0.0  \n",
       "1_1001      what else? sell it on ebay as \"slightly used.\"       21      0.0  \n",
       "1_1003    good idea, sit with her. hold her, comfort he...        7      1.0  \n",
       "1_10190   well, now that i have given up string theory,...       25      0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4a2cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (961, 3) (961, 1) Test:  ((241, 3), (241, 1))\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "data[\"target_\"] = cls + space + data[\"target\"].astype(str) + space + sep\n",
    "data[\"target_context\"] = cls + space + data[\"target\"].astype(str) + space + data[\"context\"].astype(str) + space + sep\n",
    "# data[\"target_context\"] = cls + space + data[\"target\"].astype(str) + space + sep + space + data[\"context\"].astype(str) + space + sep\n",
    "X_train, X_test, Y_train, Y_test = get_train_test_split(data, [\"target_\", \"target_context\", \"speaker\"], [\"sarcasm\"], \"sarcasm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c5c8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_</th>\n",
       "      <th>target_context</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_S09E05_291</th>\n",
       "      <td>[CLS] I've been told it's a good way to move o...</td>\n",
       "      <td>[CLS] I've been told it's a good way to move o...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_S12E07_179</th>\n",
       "      <td>[CLS] Yeah, sure. You slept with your husband....</td>\n",
       "      <td>[CLS] Yeah, sure. You slept with your husband....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_210</th>\n",
       "      <td>[CLS] When are you coming home? [SEP]</td>\n",
       "      <td>[CLS] When are you coming home? Okay. Alright....</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_S12E02_262</th>\n",
       "      <td>[CLS] Riveting. [SEP]</td>\n",
       "      <td>[CLS] Riveting. Bingo. Then I lifted the cushi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_103</th>\n",
       "      <td>[CLS] No, this is just part of a daredevil gam...</td>\n",
       "      <td>[CLS] No, this is just part of a daredevil gam...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_267</th>\n",
       "      <td>[CLS] Really!? [SEP]</td>\n",
       "      <td>[CLS] Really!? Pa-haa!! I would love to go wit...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_447</th>\n",
       "      <td>[CLS] It was an accident. Not like I was acros...</td>\n",
       "      <td>[CLS] It was an accident. Not like I was acros...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_S11E11_182</th>\n",
       "      <td>[CLS] Oh, fun. Can I help? [SEP]</td>\n",
       "      <td>[CLS] Oh, fun. Can I help? of all the cool thi...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_S10E12_115</th>\n",
       "      <td>[CLS] Cause at the end I assumed there'd be nu...</td>\n",
       "      <td>[CLS] Cause at the end I assumed there'd be nu...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_S10E07_267</th>\n",
       "      <td>[CLS] I told you Penny was hiding his things?!...</td>\n",
       "      <td>[CLS] I told you Penny was hiding his things?!...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        target_  \\\n",
       "scene                                                             \n",
       "1_S09E05_291  [CLS] I've been told it's a good way to move o...   \n",
       "1_S12E07_179  [CLS] Yeah, sure. You slept with your husband....   \n",
       "2_210                     [CLS] When are you coming home? [SEP]   \n",
       "1_S12E02_262                              [CLS] Riveting. [SEP]   \n",
       "2_103         [CLS] No, this is just part of a daredevil gam...   \n",
       "2_267                                      [CLS] Really!? [SEP]   \n",
       "2_447         [CLS] It was an accident. Not like I was acros...   \n",
       "1_S11E11_182                   [CLS] Oh, fun. Can I help? [SEP]   \n",
       "1_S10E12_115  [CLS] Cause at the end I assumed there'd be nu...   \n",
       "1_S10E07_267  [CLS] I told you Penny was hiding his things?!...   \n",
       "\n",
       "                                                 target_context  speaker  \n",
       "scene                                                                     \n",
       "1_S09E05_291  [CLS] I've been told it's a good way to move o...       25  \n",
       "1_S12E07_179  [CLS] Yeah, sure. You slept with your husband....        1  \n",
       "2_210         [CLS] When are you coming home? Okay. Alright....       16  \n",
       "1_S12E02_262  [CLS] Riveting. Bingo. Then I lifted the cushi...        0  \n",
       "2_103         [CLS] No, this is just part of a daredevil gam...        2  \n",
       "2_267         [CLS] Really!? Pa-haa!! I would love to go wit...       20  \n",
       "2_447         [CLS] It was an accident. Not like I was acros...        2  \n",
       "1_S11E11_182  [CLS] Oh, fun. Can I help? of all the cool thi...       15  \n",
       "1_S10E12_115  [CLS] Cause at the end I assumed there'd be nu...       14  \n",
       "1_S10E07_267  [CLS] I told you Penny was hiding his things?!...        7  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67816b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a42e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensors_BERT(column, text):\n",
    "    bert_pad_len = 512\n",
    "    print(\"Tokenizing text...\")\n",
    "    logging.basicConfig(level = logging.INFO)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bhadresh-savani/bert-base-uncased-emotion\")\n",
    "    tokenized_text = [tokenizer.tokenize(x) for x in text]\n",
    "    tokenized_text = [x + ([pad] * (bert_pad_len - len(x))) for x in tokenized_text]\n",
    "    indexed_text = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]\n",
    "    segment_ids = []\n",
    "    if column == \"target_bsfuhi\": \n",
    "        for text in tokenized_text:\n",
    "            septoken_index = [i for i, x in enumerate(text) if x == sep]\n",
    "            septoken_index.sort()\n",
    "            first_index = septoken_index[0]\n",
    "            second_index = septoken_index[1]\n",
    "            segment_ids_0 = [0] * (first_index + 1)\n",
    "            segment_ids_1 = [1] * (second_index - first_index)\n",
    "            segment_ids_pad = [0] * (len(text) - second_index - 1)\n",
    "            segment_id = segment_ids_0 + segment_ids_1 + segment_ids_pad\n",
    "            segment_ids.append(segment_id)\n",
    "    else:\n",
    "        for text in tokenized_text:\n",
    "            septoken_index = [i for i, x in enumerate(text) if x == sep]\n",
    "            septoken_index.sort()\n",
    "            first_index = septoken_index[0]\n",
    "            segment_ids_1 = [1] * (first_index + 1)\n",
    "            segment_ids_pad = [0] * (len(text) - first_index - 1)\n",
    "            segment_id = segment_ids_1 + segment_ids_pad\n",
    "            segment_ids.append(segment_id)\n",
    "\n",
    "    torch_idx_text = torch.LongTensor(indexed_text)\n",
    "    torch_seg_ids = torch.LongTensor(segment_ids)\n",
    "    return tokenized_text, torch_idx_text, torch_seg_ids \n",
    "\n",
    "def get_embeddings(torch_idx_text, torch_seg_ids):\n",
    "    print(\"Getting Embeddings...\")\n",
    "    model = BertModel.from_pretrained('bhadresh-savani/bert-base-uncased-emotion', output_hidden_states = True)\n",
    "    model.eval()\n",
    "\n",
    "    torch_idx_text, torch_seg_ids = torch_idx_text.to(\"cpu\"), torch_seg_ids.to(\"cpu\")\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        bert_embeddings = []\n",
    "        for i in range(len(torch_idx_text)):\n",
    "            print(i, end = \"\\r\")\n",
    "            text_temp = torch.unsqueeze(torch_idx_text[i], dim = 0).to(device)\n",
    "            sgmt_temp = torch.unsqueeze(torch_seg_ids[i], dim = 0).to(device)\n",
    "            output = model(text_temp, sgmt_temp)\n",
    "            bert_embeddings.append(output[2])\n",
    "            del text_temp, sgmt_temp\n",
    "    del model\n",
    "  \n",
    "    return bert_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85727bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d53404bade248e29909ae7e93e8f500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932bd002745043a39ec3264623cb44e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ce36db3ebb484bbbea1cfd6368e332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b263a0eb5da42cf9aebece9200d5129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486e74ff9ff54bff86c7805da1b4bc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bhadresh-savani/bert-base-uncased-emotion were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961\n",
      "Shape is: 512 x 768\n",
      "Tokenizing text...\n",
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bhadresh-savani/bert-base-uncased-emotion were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n",
      "Shape is: 512 x 768\n",
      "Tokenizing text...\n",
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bhadresh-savani/bert-base-uncased-emotion were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961\n",
      "Shape is: 512 x 768\n",
      "Tokenizing text...\n",
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bhadresh-savani/bert-base-uncased-emotion were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n",
      "Shape is: 512 x 768\n"
     ]
    }
   ],
   "source": [
    "def create_word_embeddings(bert_embeddings):\n",
    "    final_embeds = []\n",
    "    for embed in bert_embeddings:\n",
    "        token_embeddings = torch.stack(embed, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "        token_vecs_sum = []\n",
    "        for token in token_embeddings:\n",
    "            sum_vec = torch.mean(token[-4:], dim=0)\n",
    "            token_vecs_sum.append(sum_vec)\n",
    "        final_embeds.append(token_vecs_sum)\n",
    "    \n",
    "    print(len(final_embeds))\n",
    "    print('Shape is: %d x %d' % (len(final_embeds[0]), len(final_embeds[0][0])))\n",
    "    return final_embeds\n",
    "    \n",
    "def save_embeddings(embeddings_file_path, embeddings, tokenized_text):\n",
    "    with open(embeddings_file_path, mode=\"wb\") as file:\n",
    "        pkl.dump({\"embeddings\": embeddings, \"tokenized_txt\": tokenized_text}, file, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def create_embeddings(train_cleaned_data, test_cleaned_data, column):\n",
    "    train_tokenized_text, train_torch_idx_text, train_torch_seg_ids = create_tensors_BERT(column, train_cleaned_data[column])\n",
    "    train_bert_embeddings = get_embeddings(train_torch_idx_text, train_torch_seg_ids)\n",
    "    train_bert_embeddings = create_word_embeddings(train_bert_embeddings)\n",
    "    test_tokenized_text, test_torch_idx_text, test_torch_seg_ids = create_tensors_BERT(column, test_cleaned_data[column])\n",
    "    test_bert_embeddings = get_embeddings(test_torch_idx_text, test_torch_seg_ids)\n",
    "    test_bert_embeddings = create_word_embeddings(test_bert_embeddings)\n",
    "    \n",
    "    train_embeddings_file_path = \"bert_embeddings/train_bert_emo_embeddings_\" + column + \".pkl\"\n",
    "    test_embeddings_file_path = \"bert_embeddings/test_bert_emo_embeddings_\" + column + \".pkl\"\n",
    "\n",
    "    save_embeddings(train_embeddings_file_path, train_bert_embeddings, train_tokenized_text)\n",
    "    save_embeddings(test_embeddings_file_path, test_bert_embeddings, test_tokenized_text)\n",
    "\n",
    "\n",
    "inputs = [\"target_\", \"target_context\"]\n",
    "for col in inputs:\n",
    "    create_embeddings(X_train, X_test, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25f692be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.to_csv(\"bert_embeddings/train_labels_bert_emo.csv\", index = False)\n",
    "Y_test.to_csv(\"bert_embeddings/test_labels_bert_emo.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fda179b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"bert_embeddings/train_data_bert_emo.csv\", index = False)\n",
    "X_test.to_csv(\"bert_embeddings/test_data_bert_emo.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
