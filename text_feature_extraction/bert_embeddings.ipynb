{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c87623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import contractions\n",
    "from transformers import BertTokenizer\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b596713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(data: pd.Series):\n",
    "    return data.str.lower()\n",
    "\n",
    "def remove_accented_characters(data: pd.Series):\n",
    "    return data.apply(lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\"))\n",
    "\n",
    "def remove_html_encodings(data: pd.Series):\n",
    "    return data.str.replace(r\"\\d+;\", \" \", regex=True)\n",
    "\n",
    "def remove_html_tags(data: pd.Series):\n",
    "    return data.str.replace(r\"<[a-zA-Z]+\\s?/?>\", \" \", regex=True)\n",
    "\n",
    "def remove_url(data: pd.Series):\n",
    "    return data.str.replace(r\"https?://([\\w\\-\\._]+){2,}/[\\w\\-\\.\\-/=\\+_\\?]+\", \" \", regex=True)\n",
    "\n",
    "def remove_non_alpha_characters(data: pd.Series):\n",
    "    return data.str.replace(r\"_+|\\|[^a-zA-Z0-9\\s]\", \" \", regex=True)\n",
    "\n",
    "def remove_extra_spaces(data: pd.Series):\n",
    "    return data.str.replace(r\"^\\s*|\\s\\s*\", \" \", regex=True)\n",
    "\n",
    "def fix_contractions(data: pd.Series):\n",
    "    def contraction_fixer(txt: str):\n",
    "        return \" \".join([contractions.fix(word) for word in txt.split()])\n",
    "\n",
    "    return data.apply(contraction_fixer)\n",
    "\n",
    "def remove_special_words(data: pd.Series):\n",
    "    return data.str.replace(r\"\\-[^a-zA-Z]{3}\\-\", \" \", regex=True)\n",
    "\n",
    "def get_train_test_split(model_data, x_columns, y_column, stratify_column):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        model_data[x_columns],\n",
    "        model_data[y_column],\n",
    "        train_size=0.8, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        shuffle=True,\n",
    "        stratify=model_data[stratify_column])\n",
    "    \n",
    "    print(\"Train: \",X_train.shape, Y_train.shape,\n",
    "      \"Test: \",(X_test.shape, Y_test.shape))\n",
    "    print(type(X_train))\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "cls = \"[CLS]\"\n",
    "sep = \"[SEP]\"\n",
    "pad = \"[PAD]\"\n",
    "space = \" \"\n",
    "bert_pad_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8e5c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_10004</th>\n",
       "      <td>A few months. How long have you been involved ...</td>\n",
       "      <td>And of those few months, how long have you bee...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_10009</th>\n",
       "      <td>Ah-da-da-da-da! What the hell?! Excuse me? Tha...</td>\n",
       "      <td>Let the dead man talk. So, why do you think that?</td>\n",
       "      <td>PENNY</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_1001</th>\n",
       "      <td>It's smashed beyond repair. What are you gonna...</td>\n",
       "      <td>What else? Sell it on eBay as \"slightly used.\"</td>\n",
       "      <td>RAJ</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_1003</th>\n",
       "      <td>I'm gonna go back and try talking to her again...</td>\n",
       "      <td>Good idea, sit with her. Hold her, comfort her...</td>\n",
       "      <td>HOWARD</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_10190</th>\n",
       "      <td>Sure. What's up? Leonard, I could use your ass...</td>\n",
       "      <td>Well, now that I've given up string theory, I'...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_S06E02_398</th>\n",
       "      <td>I mean, he really, really likes Pied Piper. He...</td>\n",
       "      <td>Look, we cannot take blood money.</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_S06E03_366</th>\n",
       "      <td>Right. Yeah. -we could just buy Hooli. -(laugh...</td>\n",
       "      <td>The-the same way we can buy America and everyt...</td>\n",
       "      <td>RICHARD</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_S06E05_355</th>\n",
       "      <td>I was just curious to know, like, what's it li...</td>\n",
       "      <td>Well, maybe some time when you're working on s...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_S06E06_143</th>\n",
       "      <td>-Were you gonna tell me about this? -No. You g...</td>\n",
       "      <td>I thought that was the company policy-these days.</td>\n",
       "      <td>GILFOYLE</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_S06E07_272</th>\n",
       "      <td>I realized something. But a few hours later wh...</td>\n",
       "      <td>That you're an alcoholic?</td>\n",
       "      <td>DINESH</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1202 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        context  \\\n",
       "scene                                                             \n",
       "1_10004       A few months. How long have you been involved ...   \n",
       "1_10009       Ah-da-da-da-da! What the hell?! Excuse me? Tha...   \n",
       "1_1001        It's smashed beyond repair. What are you gonna...   \n",
       "1_1003        I'm gonna go back and try talking to her again...   \n",
       "1_10190       Sure. What's up? Leonard, I could use your ass...   \n",
       "...                                                         ...   \n",
       "3_S06E02_398  I mean, he really, really likes Pied Piper. He...   \n",
       "3_S06E03_366  Right. Yeah. -we could just buy Hooli. -(laugh...   \n",
       "3_S06E05_355  I was just curious to know, like, what's it li...   \n",
       "3_S06E06_143  -Were you gonna tell me about this? -No. You g...   \n",
       "3_S06E07_272  I realized something. But a few hours later wh...   \n",
       "\n",
       "                                                         target   speaker  \\\n",
       "scene                                                                       \n",
       "1_10004       And of those few months, how long have you bee...   SHELDON   \n",
       "1_10009       Let the dead man talk. So, why do you think that?     PENNY   \n",
       "1_1001           What else? Sell it on eBay as \"slightly used.\"       RAJ   \n",
       "1_1003        Good idea, sit with her. Hold her, comfort her...    HOWARD   \n",
       "1_10190       Well, now that I've given up string theory, I'...   SHELDON   \n",
       "...                                                         ...       ...   \n",
       "3_S06E02_398                  Look, we cannot take blood money.     OTHER   \n",
       "3_S06E03_366  The-the same way we can buy America and everyt...   RICHARD   \n",
       "3_S06E05_355  Well, maybe some time when you're working on s...     OTHER   \n",
       "3_S06E06_143  I thought that was the company policy-these days.  GILFOYLE   \n",
       "3_S06E07_272                          That you're an alcoholic?    DINESH   \n",
       "\n",
       "              sarcasm  \n",
       "scene                  \n",
       "1_10004           0.0  \n",
       "1_10009           0.0  \n",
       "1_1001            0.0  \n",
       "1_1003            1.0  \n",
       "1_10190           0.0  \n",
       "...               ...  \n",
       "3_S06E02_398      0.0  \n",
       "3_S06E03_366      1.0  \n",
       "3_S06E05_355      1.0  \n",
       "3_S06E06_143      1.0  \n",
       "3_S06E07_272      1.0  \n",
       "\n",
       "[1202 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"text_data_compiled.csv\")\n",
    "data = data.drop(columns=['key', 'show', 'sarcasm_type'])\n",
    "data.set_index('scene', inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf0bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"target_\"] = cls + space + data[\"target\"].astype(str) + space + sep\n",
    "data[\"target_speaker\"] = cls + space + data[\"target\"].astype(str) + space + sep + space + data[\"speaker\"].astype(str) + space + sep\n",
    "data[\"target_context\"] = cls + space + data[\"target\"].astype(str) + space + sep + space + data[\"context\"].astype(str) + space + sep\n",
    "data[\"target_context_speaker\"] = cls + space + data[\"target\"].astype(str) + space + sep + space + data[\"context\"].astype(str) + space + sep + space + data[\"speaker\"].astype(str) + space + sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee68bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (961, 4) (961, 1) Test:  ((241, 4), (241, 1))\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = get_train_test_split(data, [\"target_\", \"target_speaker\", \"target_context\", \"target_context_speaker\"], [\"sarcasm\"], \"sarcasm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "574b4b0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_cleaning_pipeline = [\n",
    "        to_lower,\n",
    "        remove_special_words,\n",
    "        remove_accented_characters,\n",
    "        remove_html_encodings,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        fix_contractions,\n",
    "        remove_non_alpha_characters,\n",
    "        remove_extra_spaces]\n",
    "\n",
    "inputs = [\"target_\", \"target_speaker\", \"target_context\", \"target_context_speaker\"]\n",
    "\n",
    "def clean_data(data):\n",
    "    data_copy = data.copy()\n",
    "    for col in inputs:\n",
    "        temp_data = data_copy[col].copy()\n",
    "        for func in data_cleaning_pipeline:\n",
    "            temp_data = func(temp_data)\n",
    "        data_copy[col] = temp_data.copy()\n",
    "    return data_copy\n",
    "\n",
    "train_cleaned_data = clean_data(X_train)\n",
    "test_cleaned_data = clean_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90606ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_</th>\n",
       "      <th>target_speaker</th>\n",
       "      <th>target_context</th>\n",
       "      <th>target_context_speaker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_S09E05_291</th>\n",
       "      <td>[cls] i have been told it is a good way to mo...</td>\n",
       "      <td>[cls] i have been told it is a good way to mo...</td>\n",
       "      <td>[cls] i have been told it is a good way to mo...</td>\n",
       "      <td>[cls] i have been told it is a good way to mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_S12E07_179</th>\n",
       "      <td>[cls] yeah, sure. you slept with your husband...</td>\n",
       "      <td>[cls] yeah, sure. you slept with your husband...</td>\n",
       "      <td>[cls] yeah, sure. you slept with your husband...</td>\n",
       "      <td>[cls] yeah, sure. you slept with your husband...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_210</th>\n",
       "      <td>[cls] when are you coming home? [sep]</td>\n",
       "      <td>[cls] when are you coming home? [sep] person ...</td>\n",
       "      <td>[cls] when are you coming home? [sep] okay. a...</td>\n",
       "      <td>[cls] when are you coming home? [sep] okay. a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_S12E02_262</th>\n",
       "      <td>[cls] riveting. [sep]</td>\n",
       "      <td>[cls] riveting. [sep] amy [sep]</td>\n",
       "      <td>[cls] riveting. [sep] bingo. then i lifted th...</td>\n",
       "      <td>[cls] riveting. [sep] bingo. then i lifted th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_103</th>\n",
       "      <td>[cls] no, this is just part of a daredevil ga...</td>\n",
       "      <td>[cls] no, this is just part of a daredevil ga...</td>\n",
       "      <td>[cls] no, this is just part of a daredevil ga...</td>\n",
       "      <td>[cls] no, this is just part of a daredevil ga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        target_  \\\n",
       "scene                                                             \n",
       "1_S09E05_291   [cls] i have been told it is a good way to mo...   \n",
       "1_S12E07_179   [cls] yeah, sure. you slept with your husband...   \n",
       "2_210                     [cls] when are you coming home? [sep]   \n",
       "1_S12E02_262                              [cls] riveting. [sep]   \n",
       "2_103          [cls] no, this is just part of a daredevil ga...   \n",
       "\n",
       "                                                 target_speaker  \\\n",
       "scene                                                             \n",
       "1_S09E05_291   [cls] i have been told it is a good way to mo...   \n",
       "1_S12E07_179   [cls] yeah, sure. you slept with your husband...   \n",
       "2_210          [cls] when are you coming home? [sep] person ...   \n",
       "1_S12E02_262                    [cls] riveting. [sep] amy [sep]   \n",
       "2_103          [cls] no, this is just part of a daredevil ga...   \n",
       "\n",
       "                                                 target_context  \\\n",
       "scene                                                             \n",
       "1_S09E05_291   [cls] i have been told it is a good way to mo...   \n",
       "1_S12E07_179   [cls] yeah, sure. you slept with your husband...   \n",
       "2_210          [cls] when are you coming home? [sep] okay. a...   \n",
       "1_S12E02_262   [cls] riveting. [sep] bingo. then i lifted th...   \n",
       "2_103          [cls] no, this is just part of a daredevil ga...   \n",
       "\n",
       "                                         target_context_speaker  \n",
       "scene                                                            \n",
       "1_S09E05_291   [cls] i have been told it is a good way to mo...  \n",
       "1_S12E07_179   [cls] yeah, sure. you slept with your husband...  \n",
       "2_210          [cls] when are you coming home? [sep] okay. a...  \n",
       "1_S12E02_262   [cls] riveting. [sep] bingo. then i lifted th...  \n",
       "2_103          [cls] no, this is just part of a daredevil ga...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc2dc6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_</th>\n",
       "      <th>target_speaker</th>\n",
       "      <th>target_context</th>\n",
       "      <th>target_context_speaker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2_388</th>\n",
       "      <td>[cls] yeah, she could not live without the ch...</td>\n",
       "      <td>[cls] yeah, she could not live without the ch...</td>\n",
       "      <td>[cls] yeah, she could not live without the ch...</td>\n",
       "      <td>[cls] yeah, she could not live without the ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_5058</th>\n",
       "      <td>[cls] an entire dinner to talk about your res...</td>\n",
       "      <td>[cls] an entire dinner to talk about your res...</td>\n",
       "      <td>[cls] an entire dinner to talk about your res...</td>\n",
       "      <td>[cls] an entire dinner to talk about your res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_S11E21_080</th>\n",
       "      <td>[cls] is it your teen years? [sep]</td>\n",
       "      <td>[cls] is it your teen years? [sep] howard [sep]</td>\n",
       "      <td>[cls] is it your teen years? [sep] no, there ...</td>\n",
       "      <td>[cls] is it your teen years? [sep] no, there ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_S11E12_038</th>\n",
       "      <td>[cls] that is funny. i always thought howard ...</td>\n",
       "      <td>[cls] that is funny. i always thought howard ...</td>\n",
       "      <td>[cls] that is funny. i always thought howard ...</td>\n",
       "      <td>[cls] that is funny. i always thought howard ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_S11E01_337</th>\n",
       "      <td>[cls] i am sorry, what? [sep]</td>\n",
       "      <td>[cls] i am sorry, what? [sep] penny [sep]</td>\n",
       "      <td>[cls] i am sorry, what? [sep] you could have ...</td>\n",
       "      <td>[cls] i am sorry, what? [sep] you could have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        target_  \\\n",
       "scene                                                             \n",
       "2_388          [cls] yeah, she could not live without the ch...   \n",
       "1_5058         [cls] an entire dinner to talk about your res...   \n",
       "1_S11E21_080                 [cls] is it your teen years? [sep]   \n",
       "1_S11E12_038   [cls] that is funny. i always thought howard ...   \n",
       "1_S11E01_337                      [cls] i am sorry, what? [sep]   \n",
       "\n",
       "                                                 target_speaker  \\\n",
       "scene                                                             \n",
       "2_388          [cls] yeah, she could not live without the ch...   \n",
       "1_5058         [cls] an entire dinner to talk about your res...   \n",
       "1_S11E21_080    [cls] is it your teen years? [sep] howard [sep]   \n",
       "1_S11E12_038   [cls] that is funny. i always thought howard ...   \n",
       "1_S11E01_337          [cls] i am sorry, what? [sep] penny [sep]   \n",
       "\n",
       "                                                 target_context  \\\n",
       "scene                                                             \n",
       "2_388          [cls] yeah, she could not live without the ch...   \n",
       "1_5058         [cls] an entire dinner to talk about your res...   \n",
       "1_S11E21_080   [cls] is it your teen years? [sep] no, there ...   \n",
       "1_S11E12_038   [cls] that is funny. i always thought howard ...   \n",
       "1_S11E01_337   [cls] i am sorry, what? [sep] you could have ...   \n",
       "\n",
       "                                         target_context_speaker  \n",
       "scene                                                            \n",
       "2_388          [cls] yeah, she could not live without the ch...  \n",
       "1_5058         [cls] an entire dinner to talk about your res...  \n",
       "1_S11E21_080   [cls] is it your teen years? [sep] no, there ...  \n",
       "1_S11E12_038   [cls] that is funny. i always thought howard ...  \n",
       "1_S11E01_337   [cls] i am sorry, what? [sep] you could have ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67816b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a42e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensors_BERT(text):\n",
    "    print(\"Tokenizing text...\")\n",
    "    logging.basicConfig(level = logging.INFO)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenized_text = [tokenizer.tokenize(x) for x in text]\n",
    "    tokenized_text = [x + ([pad] * (bert_pad_len - len(x))) for x in tokenized_text]\n",
    "    indexed_text = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]\n",
    "    segment_ids = [[1] * len(x) for x in tokenized_text]\n",
    "    torch_idx_text = torch.LongTensor(indexed_text)\n",
    "    torch_seg_ids = torch.LongTensor(segment_ids)\n",
    "    return tokenized_text, torch_idx_text, torch_seg_ids \n",
    "\n",
    "def get_embeddings(torch_idx_text, torch_seg_ids):\n",
    "    print(\"Getting Embeddings...\")\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "    model.eval()\n",
    "\n",
    "    torch_idx_text, torch_seg_ids = torch_idx_text.to(\"cpu\"), torch_seg_ids.to(\"cpu\")\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        bert_embeddings = []\n",
    "        for i in range(len(torch_idx_text)):\n",
    "            print(i, end = \"\\r\")\n",
    "            text_temp = torch.unsqueeze(torch_idx_text[i], dim = 0).to(device)\n",
    "            sgmt_temp = torch.unsqueeze(torch_seg_ids[i], dim = 0).to(device)\n",
    "            output = model(text_temp, sgmt_temp)\n",
    "            bert_embeddings.append(output[0])\n",
    "            del text_temp, sgmt_temp\n",
    "    del model\n",
    "  \n",
    "    return bert_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85727bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Tokenizing text...\n",
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961 torch.Size([1, 512, 768])\n",
      "241 torch.Size([1, 512, 768])\n",
      "torch.Size([961, 768])\n",
      "torch.Size([241, 768])\n",
      "Tokenizing text...\n",
      "Tokenizing text...\n",
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961 torch.Size([1, 512, 768])\n",
      "241 torch.Size([1, 512, 768])\n",
      "torch.Size([961, 768])\n",
      "torch.Size([241, 768])\n",
      "Tokenizing text...\n",
      "Tokenizing text...\n",
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961 torch.Size([1, 512, 768])\n",
      "241 torch.Size([1, 512, 768])\n",
      "torch.Size([961, 768])\n",
      "torch.Size([241, 768])\n",
      "Tokenizing text...\n",
      "Tokenizing text...\n",
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961 torch.Size([1, 512, 768])\n",
      "241 torch.Size([1, 512, 768])\n",
      "torch.Size([961, 768])\n",
      "torch.Size([241, 768])\n"
     ]
    }
   ],
   "source": [
    "def save_embeddings(embeddings_file_path, embeddings, tokenized_text):\n",
    "    with open(embeddings_file_path, mode=\"wb\") as file:\n",
    "        pkl.dump({\"embeddings\": embeddings, \"tokenized_txt\": tokenized_text}, file, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def create_embeddings(train_cleaned_data, test_cleaned_data, column):\n",
    "    train_tokenized_text, train_torch_idx_text, train_torch_seg_ids = create_tensors_BERT(train_cleaned_data[column])\n",
    "    test_tokenized_text, test_torch_idx_text, test_torch_seg_ids = create_tensors_BERT(test_cleaned_data[column])\n",
    "    train_bert_embeddings = get_embeddings(train_torch_idx_text, train_torch_seg_ids)\n",
    "    test_bert_embeddings = get_embeddings(test_torch_idx_text, test_torch_seg_ids)\n",
    "    train_embeddings_file_path = \"embeddings/train_bert_embeddings_\" + column + \".pkl\"\n",
    "    test_embeddings_file_path = \"embeddings/test_bert_embeddings_\" + column + \".pkl\"\n",
    "\n",
    "\n",
    "    save_embeddings(train_embeddings_file_path, train_bert_embeddings, train_tokenized_text)\n",
    "    save_embeddings(test_embeddings_file_path, test_bert_embeddings, test_tokenized_text)\n",
    "\n",
    "    print(len(train_bert_embeddings), train_bert_embeddings[0].shape)\n",
    "    print(len(test_bert_embeddings), test_bert_embeddings[0].shape)\n",
    "\n",
    "    train_bert_embeddings = torch.cat(train_bert_embeddings)\n",
    "    test_bert_embeddings = torch.cat(test_bert_embeddings)\n",
    "    train_avg_embeddings = torch.sum(train_bert_embeddings, dim=1) / 512\n",
    "    test_avg_embeddings = torch.sum(test_bert_embeddings, dim=1) / 512\n",
    "\n",
    "    train_avg_embeddings_file_path = \"embeddings/train_avg_embeddings_\" + column + \".pkl\"\n",
    "    test_avg_embeddings_file_path = \"embeddings/test_avg_embeddings_\" + column + \".pkl\"\n",
    "\n",
    "    with open(train_avg_embeddings_file_path, mode=\"wb\") as train_file, open(test_avg_embeddings_file_path, mode=\"wb\") as test_file:\n",
    "        pkl.dump(train_avg_embeddings.numpy(), train_file, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "        pkl.dump(test_avg_embeddings.numpy(), test_file, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(train_avg_embeddings.shape)\n",
    "    print(test_avg_embeddings.shape)\n",
    "\n",
    "for col in inputs:\n",
    "    create_embeddings(train_cleaned_data, test_cleaned_data, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f692be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.to_csv(\"embeddings/train_labels.csv\", index = False)\n",
    "Y_test.to_csv(\"embeddings/test_labels.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
