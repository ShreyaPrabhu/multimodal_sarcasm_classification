{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42e3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70375242",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"../../data/scene_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776a1a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCENE</th>\n",
       "      <th>KEY</th>\n",
       "      <th>SHOW</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Sarcasm_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_10004</td>\n",
       "      <td>1_10004_u</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_10009</td>\n",
       "      <td>1_10009_u</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1001</td>\n",
       "      <td>1_1001_u</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1003</td>\n",
       "      <td>1_1003_u</td>\n",
       "      <td>BBT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_10190</td>\n",
       "      <td>1_10190_u</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SCENE        KEY SHOW  Sarcasm Sarcasm_Type\n",
       "0  1_10004  1_10004_u  BBT      0.0         NONE\n",
       "1  1_10009  1_10009_u  BBT      0.0         NONE\n",
       "2   1_1001   1_1001_u  BBT      0.0         NONE\n",
       "3   1_1003   1_1003_u  BBT      1.0          PRO\n",
       "4  1_10190  1_10190_u  BBT      0.0         NONE"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a947217",
   "metadata": {},
   "source": [
    "#### Perform mean, median, max, min and sum pooling on audio feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9bd19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(audio_features):\n",
    "    model_data = pd.DataFrame(columns=['audio_feature','sarcasm','sarcasm_type'])\n",
    "    for index, row in labels.iterrows():\n",
    "        audio_key = row[\"SCENE\"] + \"_u.wav\"\n",
    "        model_data = model_data.append({'audio_feature': audio_features[audio_key],\n",
    "                                    'sarcasm' : row[\"Sarcasm\"],\n",
    "                                    'sarcasm_type' : row[\"Sarcasm_Type\"]},\n",
    "                                  ignore_index=True)\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac0ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(model_data, x_column, y_column, stratify_column):\n",
    "    model_data = model_data[model_data[y_column] != \"NONE\"]\n",
    "    model_data = model_data[model_data[y_column] != \"LIK\"]\n",
    "    train_data = model_data.groupby(stratify_column).apply(lambda x: x.sample(frac=0.8, random_state=42))\n",
    "    train_index_tuples = train_data.index.values.tolist()\n",
    "    train_index_list = []\n",
    "    for tup in train_index_tuples:\n",
    "        train_index_list.append(tup[1])\n",
    "    test_data = model_data[~model_data.index.isin(train_index_list)]\n",
    "    train_data.reset_index(drop=True, inplace = True)\n",
    "    test_data.reset_index(drop=True, inplace = True)\n",
    "    print(train_data.sarcasm_type.value_counts())\n",
    "    print(test_data.sarcasm_type.value_counts())\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4efd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding = {\"PRO\" : 0, \"ILL\" : 1, \"EMB\" : 2}\n",
    "\n",
    "class GRUTensorDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data.loc[index, 'padded_audio_feature']\n",
    "        label = self.data.loc[index, 'sarcasm_type']\n",
    "        return torch.from_numpy(features).float(), label_encoding[label]\n",
    "    \n",
    "    def __getindexlist__(self):\n",
    "        return list(self.data.index.values)\n",
    "    \n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, \n",
    "                 output_dim, n_layers):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, \n",
    "                          n_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.softmax(self.fc(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, \n",
    "                            self.hidden_dim).zero_()\n",
    "        return hidden\n",
    "    \n",
    "def evaluateGRU(gru, review, size):\n",
    "    hidden = gru.init_hidden(size)\n",
    "    output, hidden = gru(review, hidden)\n",
    "    return output\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = torch.max(output,dim=1)\n",
    "    return top_i\n",
    "\n",
    "def test_accuracy(gru, loader, size):\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    for data, target in loader:\n",
    "        output = evaluateGRU(gru, data, size)\n",
    "        prediction_index = categoryFromOutput(output)\n",
    "        predictions = prediction_index.tolist()\n",
    "        actuals = target.tolist()\n",
    "    return predictions, actuals\n",
    "    \n",
    "hidden_size = 18\n",
    "output_size = 3\n",
    "input_size = 690\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3c666",
   "metadata": {},
   "source": [
    "### Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d644d735",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO    266\n",
      "ILL    142\n",
      "EMB     70\n",
      "Name: sarcasm_type, dtype: int64\n",
      "PRO    67\n",
      "ILL    36\n",
      "EMB    17\n",
      "Name: sarcasm_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with open('../../audio_features/feat_dict_librosa_lld.pickle', 'rb') as f:\n",
    "    librosa_audio_features = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "model_data = get_model_data(librosa_audio_features)\n",
    "\n",
    "desired_length = 18\n",
    "\n",
    "train_data, test_data = get_train_test_split(model_data, 'audio_feature', 'sarcasm_type', 'sarcasm_type')\n",
    "fnn_train = train_data.copy()\n",
    "fnn_test = test_data.copy()\n",
    "fnn_train.reset_index(drop=True, inplace = True)\n",
    "fnn_test.reset_index(drop=True, inplace = True)\n",
    "\n",
    "        \n",
    "fnn_train['padded_audio_feature'] = fnn_train.loc[:, 'audio_feature']\n",
    "for index, row in fnn_train.iterrows():\n",
    "    data_array = row['padded_audio_feature']\n",
    "    new_array = []\n",
    "    for arr in data_array:\n",
    "        if arr.shape[0] < desired_length:\n",
    "            arr = np.pad(arr, (0, desired_length - arr.shape[0]), 'constant')\n",
    "            new_array.append(arr)\n",
    "        else:\n",
    "            new_array.append(arr)\n",
    "    fnn_train.at[index, \"padded_audio_feature\"] = np.transpose(np.array(new_array))\n",
    "\n",
    "fnn_test['padded_audio_feature'] = fnn_test.loc[:, 'audio_feature']\n",
    "for index, row in fnn_test.iterrows():\n",
    "    data_array = row['padded_audio_feature']\n",
    "    new_array = []\n",
    "    for arr in data_array:\n",
    "        if arr.shape[0] < desired_length:\n",
    "            arr = np.pad(arr, (0, desired_length - arr.shape[0]), 'constant')\n",
    "            new_array.append(arr)\n",
    "        else:\n",
    "            new_array.append(arr)\n",
    "    fnn_test.at[index, \"padded_audio_feature\"] = np.transpose(np.array(new_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a06ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_train_tensor = GRUTensorDataset(fnn_train)\n",
    "fnn_test_tensor = GRUTensorDataset(fnn_test)\n",
    "\n",
    "num_of_workers = 0\n",
    "batch_size = 48\n",
    "valid_size = 0.1\n",
    "\n",
    "train_indices = list(range(len(fnn_train_tensor)))\n",
    "np.random.shuffle(train_indices)\n",
    "\n",
    "test_indices = list(range(len(fnn_test_tensor)))\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    fnn_train_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    fnn_test_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(test_indices),\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e40f4fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUNet(\n",
      "  (gru): GRU(690, 18, batch_first=True)\n",
      "  (fc): Linear(in_features=18, out_features=3, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "[2.27619048 1.12206573 0.59899749]\n"
     ]
    }
   ],
   "source": [
    "gru = GRUNet(input_size, hidden_size, output_size, n_layers)\n",
    "print(gru)\n",
    "\n",
    "\n",
    "class_weight = compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(fnn_train[\"sarcasm_type\"]), y=fnn_train[\"sarcasm_type\"]\n",
    ")\n",
    "print(class_weight)\n",
    "\n",
    "criterion = nn.NLLLoss(weight=torch.FloatTensor(class_weight))\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d6606eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "              precision    recall  f1-score     support\n",
      "0              0.000000  0.000000  0.000000   67.000000\n",
      "1              0.000000  0.000000  0.000000   36.000000\n",
      "2              0.141667  1.000000  0.248175   17.000000\n",
      "accuracy       0.141667  0.141667  0.141667    0.141667\n",
      "macro avg      0.047222  0.333333  0.082725  120.000000\n",
      "weighted avg   0.020069  0.141667  0.035158  120.000000\n",
      "Epoch: 50\n",
      "              precision    recall  f1-score     support\n",
      "0              0.558333  1.000000  0.716578   67.000000\n",
      "1              0.000000  0.000000  0.000000   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.186111  0.333333  0.238859  120.000000\n",
      "weighted avg   0.311736  0.558333  0.400089  120.000000\n",
      "Epoch: 100\n",
      "              precision    recall  f1-score     support\n",
      "0              0.558333  1.000000  0.716578   67.000000\n",
      "1              0.000000  0.000000  0.000000   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.186111  0.333333  0.238859  120.000000\n",
      "weighted avg   0.311736  0.558333  0.400089  120.000000\n",
      "Epoch: 150\n",
      "              precision    recall  f1-score     support\n",
      "0              0.558333  1.000000  0.716578   67.000000\n",
      "1              0.000000  0.000000  0.000000   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.186111  0.333333  0.238859  120.000000\n",
      "weighted avg   0.311736  0.558333  0.400089  120.000000\n",
      "Epoch: 200\n",
      "              precision    recall  f1-score     support\n",
      "0              0.558333  1.000000  0.716578   67.000000\n",
      "1              0.000000  0.000000  0.000000   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.186111  0.333333  0.238859  120.000000\n",
      "weighted avg   0.311736  0.558333  0.400089  120.000000\n",
      "Epoch: 250\n",
      "              precision    recall  f1-score     support\n",
      "0              0.561404  0.955224  0.707182   67.000000\n",
      "1              0.166667  0.027778  0.047619   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.242690  0.327667  0.251600  120.000000\n",
      "weighted avg   0.363450  0.541667  0.409129  120.000000\n",
      "Epoch: 300\n",
      "              precision    recall  f1-score     support\n",
      "0              0.549451  0.746269  0.632911   67.000000\n",
      "1              0.206897  0.166667  0.184615   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.466667  0.466667  0.466667    0.466667\n",
      "macro avg      0.252116  0.304312  0.272509  120.000000\n",
      "weighted avg   0.368846  0.466667  0.408760  120.000000\n",
      "Epoch: 350\n",
      "              precision    recall  f1-score     support\n",
      "0              0.550562  0.731343  0.628205   67.000000\n",
      "1              0.200000  0.166667  0.181818   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.458333  0.458333  0.458333    0.458333\n",
      "macro avg      0.250187  0.299337  0.270008  120.000000\n",
      "weighted avg   0.367397  0.458333  0.405293  120.000000\n",
      "Epoch: 400\n",
      "              precision    recall  f1-score     support\n",
      "0              0.557895  0.791045  0.654321   67.000000\n",
      "1              0.208333  0.138889  0.166667   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.483333  0.483333  0.483333    0.483333\n",
      "macro avg      0.255409  0.309978  0.273663  120.000000\n",
      "weighted avg   0.373991  0.483333  0.415329  120.000000\n",
      "Epoch: 450\n",
      "              precision    recall  f1-score  support\n",
      "0              0.578313  0.716418  0.640000   67.000\n",
      "1              0.250000  0.250000  0.250000   36.000\n",
      "2              0.000000  0.000000  0.000000   17.000\n",
      "accuracy       0.475000  0.475000  0.475000    0.475\n",
      "macro avg      0.276104  0.322139  0.296667  120.000\n",
      "weighted avg   0.397892  0.475000  0.432333  120.000\n",
      "Epoch: 500\n",
      "              precision    recall  f1-score     support\n",
      "0              0.576471  0.731343  0.644737   67.000000\n",
      "1              0.264706  0.250000  0.257143   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.483333  0.483333  0.483333    0.483333\n",
      "macro avg      0.280392  0.327114  0.300627  120.000000\n",
      "weighted avg   0.401275  0.483333  0.437121  120.000000\n",
      "Epoch: 550\n",
      "              precision    recall  f1-score     support\n",
      "0              0.576471  0.731343  0.644737   67.000000\n",
      "1              0.264706  0.250000  0.257143   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.483333  0.483333  0.483333    0.483333\n",
      "macro avg      0.280392  0.327114  0.300627  120.000000\n",
      "weighted avg   0.401275  0.483333  0.437121  120.000000\n",
      "Epoch: 600\n",
      "              precision    recall  f1-score  support\n",
      "0              0.578313  0.716418  0.640000   67.000\n",
      "1              0.250000  0.250000  0.250000   36.000\n",
      "2              0.000000  0.000000  0.000000   17.000\n",
      "accuracy       0.475000  0.475000  0.475000    0.475\n",
      "macro avg      0.276104  0.322139  0.296667  120.000\n",
      "weighted avg   0.397892  0.475000  0.432333  120.000\n",
      "Epoch: 650\n",
      "              precision    recall  f1-score  support\n",
      "0              0.563218  0.731343  0.636364   67.000\n",
      "1              0.250000  0.222222  0.235294   36.000\n",
      "2              0.000000  0.000000  0.000000   17.000\n",
      "accuracy       0.475000  0.475000  0.475000    0.475\n",
      "macro avg      0.271073  0.317855  0.290553  120.000\n",
      "weighted avg   0.389464  0.475000  0.425891  120.000\n",
      "Epoch: 700\n",
      "              precision    recall  f1-score     support\n",
      "0              0.578313  0.716418  0.640000   67.000000\n",
      "1              0.310345  0.250000  0.276923   36.000000\n",
      "2              0.125000  0.058824  0.080000   17.000000\n",
      "accuracy       0.483333  0.483333  0.483333    0.483333\n",
      "macro avg      0.337886  0.341747  0.332308  120.000000\n",
      "weighted avg   0.433703  0.483333  0.451744  120.000000\n",
      "Epoch: 750\n",
      "              precision    recall  f1-score     support\n",
      "0              0.589474  0.835821  0.691358   67.000000\n",
      "1              0.333333  0.222222  0.266667   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.533333  0.533333  0.533333    0.533333\n",
      "macro avg      0.307602  0.352681  0.319342  120.000000\n",
      "weighted avg   0.429123  0.533333  0.466008  120.000000\n",
      "Epoch: 800\n",
      "              precision    recall  f1-score  support\n",
      "0              0.585106  0.820896  0.683230   67.000\n",
      "1              0.320000  0.222222  0.262295   36.000\n",
      "2              0.000000  0.000000  0.000000   17.000\n",
      "accuracy       0.525000  0.525000  0.525000    0.525\n",
      "macro avg      0.301702  0.347706  0.315175  120.000\n",
      "weighted avg   0.422684  0.525000  0.460159  120.000\n",
      "Epoch: 850\n",
      "              precision    recall  f1-score  support\n",
      "0              0.571429  0.776119  0.658228     67.0\n",
      "1              0.285714  0.222222  0.250000     36.0\n",
      "2              0.000000  0.000000  0.000000     17.0\n",
      "accuracy       0.500000  0.500000  0.500000      0.5\n",
      "macro avg      0.285714  0.332781  0.302743    120.0\n",
      "weighted avg   0.404762  0.500000  0.442511    120.0\n",
      "Epoch: 900\n",
      "              precision    recall  f1-score     support\n",
      "0              0.585106  0.820896  0.683230   67.000000\n",
      "1              0.360000  0.250000  0.295082   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.533333  0.533333  0.533333    0.533333\n",
      "macro avg      0.315035  0.356965  0.326104  120.000000\n",
      "weighted avg   0.434684  0.533333  0.469995  120.000000\n",
      "Epoch: 950\n",
      "              precision    recall  f1-score  support\n",
      "0              0.578947  0.820896  0.679012   67.000\n",
      "1              0.333333  0.222222  0.266667   36.000\n",
      "2              0.000000  0.000000  0.000000   17.000\n",
      "accuracy       0.525000  0.525000  0.525000    0.525\n",
      "macro avg      0.304094  0.347706  0.315226  120.000\n",
      "weighted avg   0.423246  0.525000  0.459115  120.000\n",
      "Epoch: 1000\n",
      "              precision    recall  f1-score  support\n",
      "0              0.578947  0.820896  0.679012   67.000\n",
      "1              0.333333  0.222222  0.266667   36.000\n",
      "2              0.000000  0.000000  0.000000   17.000\n",
      "accuracy       0.525000  0.525000  0.525000    0.525\n",
      "macro avg      0.304094  0.347706  0.315226  120.000\n",
      "weighted avg   0.423246  0.525000  0.459115  120.000\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1001\n",
    "    \n",
    "test_min_loss = np.inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.manual_seed(42)\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    gru.train()\n",
    "    for data, target in train_loader:\n",
    "        h = gru.init_hidden(batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output, h = gru(data, h.data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    gru.eval()\n",
    "    for data, target in test_loader:\n",
    "        if data.shape[1] < 48:\n",
    "            continue\n",
    "        h = gru.init_hidden(batch_size)\n",
    "        output, h = gru(data, h.data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()*data.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "#     if(epoch%20 == 0):\n",
    "# #         print(f\"Epoch: {epoch+1:02}\")\n",
    "# #         print(\"\\tTraining Loss: {:.6f} \\Test Loss: {:.6f}\".format(train_loss, test_loss))\n",
    "#     if test_loss <= test_min_loss:\n",
    "# #         print(\"Test loss decreased ({:.6f} --> {:.6f}). Saving model...\".format(test_min_loss, test_loss))\n",
    "# #         torch.save(gru.state_dict(), \"fnnmodel.pt\")\n",
    "#         test_min_loss = test_loss\n",
    "    if(epoch%50 == 0):\n",
    "        torch.save(gru.state_dict(), \"gru/gru_\" + str(epoch) + \"_.pt\")\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        test_loader = torch.utils.data.DataLoader(fnn_test_tensor, batch_size=fnn_test_tensor.__len__())\n",
    "        predictions, actuals = test_accuracy(gru, test_loader, fnn_test_tensor.__len__())\n",
    "        print(pd.DataFrame(classification_report(actuals, predictions, output_dict=True)).T)\n",
    "        test_loader = torch.utils.data.DataLoader(fnn_test_tensor, batch_size=batch_size, sampler=SubsetRandomSampler(test_indices),drop_last=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
