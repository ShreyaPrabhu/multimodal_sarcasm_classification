{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42e3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70375242",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"../../data/scene_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776a1a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCENE</th>\n",
       "      <th>KEY</th>\n",
       "      <th>SPEAKER</th>\n",
       "      <th>SHOW</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Sarcasm_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_10004</td>\n",
       "      <td>1_10004_u</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_10009</td>\n",
       "      <td>1_10009_u</td>\n",
       "      <td>PENNY</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1001</td>\n",
       "      <td>1_1001_u</td>\n",
       "      <td>RAJ</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1003</td>\n",
       "      <td>1_1003_u</td>\n",
       "      <td>HOWARD</td>\n",
       "      <td>BBT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_10190</td>\n",
       "      <td>1_10190_u</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SCENE        KEY  SPEAKER SHOW  Sarcasm Sarcasm_Type\n",
       "0  1_10004  1_10004_u  SHELDON  BBT      0.0         NONE\n",
       "1  1_10009  1_10009_u    PENNY  BBT      0.0         NONE\n",
       "2   1_1001   1_1001_u      RAJ  BBT      0.0         NONE\n",
       "3   1_1003   1_1003_u   HOWARD  BBT      1.0          PRO\n",
       "4  1_10190  1_10190_u  SHELDON  BBT      0.0         NONE"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a947217",
   "metadata": {},
   "source": [
    "#### Perform mean, median, max, min and sum pooling on audio feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9bd19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(audio_features):\n",
    "    model_data = pd.DataFrame(columns=['audio_feature','sarcasm','sarcasm_type', 'speaker'])\n",
    "    for index, row in labels.iterrows():\n",
    "        audio_key = row[\"SCENE\"] + \"_u.wav\"\n",
    "        model_data = model_data.append({'audio_feature': audio_features[audio_key],\n",
    "                                    'sarcasm' : row[\"Sarcasm\"],\n",
    "                                    'sarcasm_type' : row[\"Sarcasm_Type\"],\n",
    "                                    'speaker' : row[\"SPEAKER\"]},\n",
    "                                  ignore_index=True)\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac0ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(model_data, x_column, y_column, stratify_column):\n",
    "    model_data = model_data[model_data[y_column] != \"NONE\"]\n",
    "    model_data = model_data[model_data[y_column] != \"LIK\"]\n",
    "    train_data = model_data.groupby(stratify_column).apply(lambda x: x.sample(frac=0.8, random_state=42))\n",
    "    train_index_tuples = train_data.index.values.tolist()\n",
    "    train_index_list = []\n",
    "    for tup in train_index_tuples:\n",
    "        train_index_list.append(tup[1])\n",
    "    test_data = model_data[~model_data.index.isin(train_index_list)]\n",
    "    train_data.reset_index(drop=True, inplace = True)\n",
    "    test_data.reset_index(drop=True, inplace = True)\n",
    "    print(train_data.sarcasm_type.value_counts())\n",
    "    print(test_data.sarcasm_type.value_counts())\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4efd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding = {\"PRO\" : 0, \"ILL\" : 1, \"EMB\" : 2}\n",
    "\n",
    "class GRUTensorDataset(Dataset):\n",
    "    def __init__(self, dataframe, speaker):\n",
    "        self.data = dataframe\n",
    "        self.speaker = speaker\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.speaker:\n",
    "            features = self.data.loc[index, 'padded_audio_feature']\n",
    "            a=np.empty((18,1))\n",
    "            a.fill(self.data.loc[index, 'speaker_encode'])\n",
    "            final_features = np.hstack((features, a))\n",
    "            label = self.data.loc[index, 'sarcasm_type']\n",
    "            return torch.from_numpy(final_features).float(), label_encoding[label]\n",
    "        else:\n",
    "            features = self.data.loc[index, 'padded_audio_feature']\n",
    "            label = self.data.loc[index, 'sarcasm_type']\n",
    "            return torch.from_numpy(features).float(), label_encoding[label]\n",
    "\n",
    "    def __getindexlist__(self):\n",
    "        return list(self.data.index.values)\n",
    "    \n",
    "\n",
    "    \n",
    "class GRUNetSD(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, \n",
    "                 output_dim, n_layers):\n",
    "        super(GRUNetSD, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, \n",
    "                          n_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.softmax(self.fc(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, \n",
    "                            self.hidden_dim).zero_()\n",
    "        return hidden\n",
    "    \n",
    "class GRUNetSID(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, \n",
    "                 output_dim, n_layers):\n",
    "        super(GRUNetSID, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, \n",
    "                          n_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.softmax(self.fc(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, \n",
    "                            self.hidden_dim).zero_()\n",
    "        return hidden\n",
    "    \n",
    "def evaluateGRU(gru, review, size):\n",
    "    hidden = gru.init_hidden(size)\n",
    "    output, hidden = gru(review, hidden)\n",
    "    return output\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = torch.max(output,dim=1)\n",
    "    return top_i\n",
    "\n",
    "def test_accuracy(gru, loader, size):\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    for data, target in loader:\n",
    "        output = evaluateGRU(gru, data, size)\n",
    "        prediction_index = categoryFromOutput(output)\n",
    "        predictions = prediction_index.tolist()\n",
    "        actuals = target.tolist()\n",
    "    return predictions, actuals\n",
    "    \n",
    "hidden_size = 18\n",
    "output_size = 3\n",
    "input_size_sid = 690\n",
    "input_size_sd = 691\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2354cbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_feature</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sarcasm_type</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_encode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-386.6164855957031, -649.6673512776692, -633...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-255.5221405029297, -484.69307309105284, -52...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>PENNY</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-569.0548095703125, -381.4147456242488, -221...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>RAJ</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-237.61074829101562, -211.002773845897, -382...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PRO</td>\n",
       "      <td>HOWARD</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-530.5701293945312, -374.83951552370763, -42...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       audio_feature  sarcasm sarcasm_type  \\\n",
       "0  [[-386.6164855957031, -649.6673512776692, -633...      0.0         NONE   \n",
       "1  [[-255.5221405029297, -484.69307309105284, -52...      0.0         NONE   \n",
       "2  [[-569.0548095703125, -381.4147456242488, -221...      0.0         NONE   \n",
       "3  [[-237.61074829101562, -211.002773845897, -382...      1.0          PRO   \n",
       "4  [[-530.5701293945312, -374.83951552370763, -42...      0.0         NONE   \n",
       "\n",
       "   speaker  speaker_encode  \n",
       "0  SHELDON              25  \n",
       "1    PENNY              15  \n",
       "2      RAJ              21  \n",
       "3   HOWARD               7  \n",
       "4  SHELDON              25  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../audio_features/feat_dict_librosa_lld.pickle', 'rb') as f:\n",
    "    librosa_audio_features = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "model_data = get_model_data(librosa_audio_features)\n",
    "le = preprocessing.LabelEncoder()\n",
    "model_data['speaker_encode'] = le.fit_transform(model_data['speaker'])\n",
    "model_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d644d735",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO    266\n",
      "ILL    142\n",
      "EMB     70\n",
      "Name: sarcasm_type, dtype: int64\n",
      "PRO    67\n",
      "ILL    36\n",
      "EMB    17\n",
      "Name: sarcasm_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "desired_length = 18\n",
    "\n",
    "train_data, test_data = get_train_test_split(model_data, 'audio_feature', 'sarcasm_type', 'sarcasm_type')\n",
    "fnn_train = train_data.copy()\n",
    "fnn_test = test_data.copy()\n",
    "fnn_train.reset_index(drop=True, inplace = True)\n",
    "fnn_test.reset_index(drop=True, inplace = True)\n",
    "\n",
    "        \n",
    "fnn_train['padded_audio_feature'] = fnn_train.loc[:, 'audio_feature']\n",
    "for index, row in fnn_train.iterrows():\n",
    "    data_array = row['padded_audio_feature']\n",
    "    new_array = []\n",
    "    for arr in data_array:\n",
    "        if arr.shape[0] < desired_length:\n",
    "            arr = np.pad(arr, (0, desired_length - arr.shape[0]), 'constant')\n",
    "            new_array.append(arr)\n",
    "        else:\n",
    "            new_array.append(arr)\n",
    "    fnn_train.at[index, \"padded_audio_feature\"] = np.transpose(np.array(new_array))\n",
    "\n",
    "fnn_test['padded_audio_feature'] = fnn_test.loc[:, 'audio_feature']\n",
    "for index, row in fnn_test.iterrows():\n",
    "    data_array = row['padded_audio_feature']\n",
    "    new_array = []\n",
    "    for arr in data_array:\n",
    "        if arr.shape[0] < desired_length:\n",
    "            arr = np.pad(arr, (0, desired_length - arr.shape[0]), 'constant')\n",
    "            new_array.append(arr)\n",
    "        else:\n",
    "            new_array.append(arr)\n",
    "    fnn_test.at[index, \"padded_audio_feature\"] = np.transpose(np.array(new_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3c666",
   "metadata": {},
   "source": [
    "### Speaker Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a06ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_train_tensor = GRUTensorDataset(fnn_train, False)\n",
    "fnn_test_tensor = GRUTensorDataset(fnn_test, False)\n",
    "\n",
    "num_of_workers = 0\n",
    "batch_size = 48\n",
    "valid_size = 0.1\n",
    "\n",
    "train_indices = list(range(len(fnn_train_tensor)))\n",
    "np.random.shuffle(train_indices)\n",
    "\n",
    "test_indices = list(range(len(fnn_test_tensor)))\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    fnn_train_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    fnn_test_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(test_indices),\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e40f4fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUNetSID(\n",
      "  (gru): GRU(690, 18, batch_first=True)\n",
      "  (fc): Linear(in_features=18, out_features=3, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "[2.27619048 1.12206573 0.59899749]\n"
     ]
    }
   ],
   "source": [
    "gru = GRUNetSID(input_size_sid, hidden_size, output_size, n_layers)\n",
    "print(gru)\n",
    "\n",
    "\n",
    "class_weight = compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(fnn_train[\"sarcasm_type\"]), y=fnn_train[\"sarcasm_type\"]\n",
    ")\n",
    "print(class_weight)\n",
    "\n",
    "criterion = nn.NLLLoss(weight=torch.FloatTensor(class_weight))\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d6606eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "              precision    recall  f1-score  support\n",
      "0                  0.00  0.000000  0.000000     67.0\n",
      "1                  0.30  1.000000  0.461538     36.0\n",
      "2                  0.00  0.000000  0.000000     17.0\n",
      "accuracy           0.30  0.300000  0.300000      0.3\n",
      "macro avg          0.10  0.333333  0.153846    120.0\n",
      "weighted avg       0.09  0.300000  0.138462    120.0\n",
      "Epoch: 50\n",
      "              precision    recall  f1-score     support\n",
      "0              0.558333  1.000000  0.716578   67.000000\n",
      "1              0.000000  0.000000  0.000000   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.186111  0.333333  0.238859  120.000000\n",
      "weighted avg   0.311736  0.558333  0.400089  120.000000\n",
      "Epoch: 100\n",
      "              precision    recall  f1-score     support\n",
      "0              0.558333  1.000000  0.716578   67.000000\n",
      "1              0.000000  0.000000  0.000000   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.186111  0.333333  0.238859  120.000000\n",
      "weighted avg   0.311736  0.558333  0.400089  120.000000\n",
      "Epoch: 150\n",
      "              precision    recall  f1-score     support\n",
      "0              0.558333  1.000000  0.716578   67.000000\n",
      "1              0.000000  0.000000  0.000000   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.186111  0.333333  0.238859  120.000000\n",
      "weighted avg   0.311736  0.558333  0.400089  120.000000\n",
      "Epoch: 200\n",
      "              precision    recall  f1-score     support\n",
      "0              0.558333  1.000000  0.716578   67.000000\n",
      "1              0.000000  0.000000  0.000000   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.186111  0.333333  0.238859  120.000000\n",
      "weighted avg   0.311736  0.558333  0.400089  120.000000\n",
      "Epoch: 250\n",
      "              precision    recall  f1-score     support\n",
      "0              0.558333  1.000000  0.716578   67.000000\n",
      "1              0.000000  0.000000  0.000000   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.186111  0.333333  0.238859  120.000000\n",
      "weighted avg   0.311736  0.558333  0.400089  120.000000\n",
      "Epoch: 300\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 350\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 400\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 450\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 500\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 550\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 600\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 650\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 700\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 750\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 800\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 850\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 900\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 950\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n",
      "Epoch: 1000\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556522  0.955224  0.703297   67.000000\n",
      "1              0.200000  0.027778  0.048780   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.541667  0.541667  0.541667    0.541667\n",
      "macro avg      0.252174  0.327667  0.250692  120.000000\n",
      "weighted avg   0.370725  0.541667  0.407308  120.000000\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1001\n",
    "    \n",
    "test_min_loss = np.inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.manual_seed(42)\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    gru.train()\n",
    "    for data, target in train_loader:\n",
    "        h = gru.init_hidden(batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output, h = gru(data, h.data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    gru.eval()\n",
    "    for data, target in test_loader:\n",
    "        if data.shape[1] < 48:\n",
    "            continue\n",
    "        h = gru.init_hidden(batch_size)\n",
    "        output, h = gru(data, h.data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()*data.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "#     if(epoch%20 == 0):\n",
    "# #         print(f\"Epoch: {epoch+1:02}\")\n",
    "# #         print(\"\\tTraining Loss: {:.6f} \\Test Loss: {:.6f}\".format(train_loss, test_loss))\n",
    "#     if test_loss <= test_min_loss:\n",
    "# #         print(\"Test loss decreased ({:.6f} --> {:.6f}). Saving model...\".format(test_min_loss, test_loss))\n",
    "# #         torch.save(gru.state_dict(), \"fnnmodel.pt\")\n",
    "#         test_min_loss = test_loss\n",
    "    if(epoch%50 == 0):\n",
    "        torch.save(gru.state_dict(), \"gru/gru_\" + str(epoch) + \"_.pt\")\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        test_loader = torch.utils.data.DataLoader(fnn_test_tensor, batch_size=fnn_test_tensor.__len__())\n",
    "        predictions, actuals = test_accuracy(gru, test_loader, fnn_test_tensor.__len__())\n",
    "        print(pd.DataFrame(classification_report(actuals, predictions, output_dict=True)).T)\n",
    "        test_loader = torch.utils.data.DataLoader(fnn_test_tensor, batch_size=batch_size, sampler=SubsetRandomSampler(test_indices),drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16037be",
   "metadata": {},
   "source": [
    "### Speaker Dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c9f2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_train_tensor = GRUTensorDataset(fnn_train, True)\n",
    "fnn_test_tensor = GRUTensorDataset(fnn_test, True)\n",
    "\n",
    "num_of_workers = 0\n",
    "batch_size = 48\n",
    "valid_size = 0.1\n",
    "\n",
    "train_indices = list(range(len(fnn_train_tensor)))\n",
    "np.random.shuffle(train_indices)\n",
    "\n",
    "test_indices = list(range(len(fnn_test_tensor)))\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    fnn_train_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    fnn_test_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(test_indices),\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef60086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUNetSD(\n",
      "  (gru): GRU(691, 18, batch_first=True)\n",
      "  (fc): Linear(in_features=18, out_features=3, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "[2.27619048 1.12206573 0.59899749]\n"
     ]
    }
   ],
   "source": [
    "gru = GRUNetSD(input_size_sd, hidden_size, output_size, n_layers)\n",
    "print(gru)\n",
    "\n",
    "\n",
    "class_weight = compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(fnn_train[\"sarcasm_type\"]), y=fnn_train[\"sarcasm_type\"]\n",
    ")\n",
    "print(class_weight)\n",
    "\n",
    "criterion = nn.NLLLoss(weight=torch.FloatTensor(class_weight))\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b52b1435",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "              precision    recall  f1-score  support\n",
      "0              0.516129  0.477612  0.496124    67.00\n",
      "1              0.000000  0.000000  0.000000    36.00\n",
      "2              0.172414  0.588235  0.266667    17.00\n",
      "accuracy       0.350000  0.350000  0.350000     0.35\n",
      "macro avg      0.229514  0.355282  0.254264   120.00\n",
      "weighted avg   0.312597  0.350000  0.314780   120.00\n",
      "Epoch: 50\n",
      "              precision    recall  f1-score  support\n",
      "0              0.554622  0.985075  0.709677    67.00\n",
      "1              0.000000  0.000000  0.000000    36.00\n",
      "2              0.000000  0.000000  0.000000    17.00\n",
      "accuracy       0.550000  0.550000  0.550000     0.55\n",
      "macro avg      0.184874  0.328358  0.236559   120.00\n",
      "weighted avg   0.309664  0.550000  0.396237   120.00\n",
      "Epoch: 100\n",
      "              precision    recall  f1-score     support\n",
      "0              0.560345  0.970149  0.710383   67.000000\n",
      "1              0.500000  0.055556  0.100000   36.000000\n",
      "2              0.000000  0.000000  0.000000   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.353448  0.341902  0.270128  120.000000\n",
      "weighted avg   0.462859  0.558333  0.426630  120.000000\n",
      "Epoch: 150\n",
      "              precision    recall  f1-score  support\n",
      "0              0.549550  0.910448  0.685393   67.000\n",
      "1              0.166667  0.027778  0.047619   36.000\n",
      "2              0.333333  0.058824  0.100000   17.000\n",
      "accuracy       0.525000  0.525000  0.525000    0.525\n",
      "macro avg      0.349850  0.332350  0.277671  120.000\n",
      "weighted avg   0.404054  0.525000  0.411130  120.000\n",
      "Epoch: 200\n",
      "              precision    recall  f1-score  support\n",
      "0              0.561905  0.880597  0.686047   67.000\n",
      "1              0.300000  0.083333  0.130435   36.000\n",
      "2              0.200000  0.058824  0.090909   17.000\n",
      "accuracy       0.525000  0.525000  0.525000    0.525\n",
      "macro avg      0.353968  0.340918  0.302463  120.000\n",
      "weighted avg   0.432063  0.525000  0.435052  120.000\n",
      "Epoch: 250\n",
      "              precision    recall  f1-score     support\n",
      "0              0.541667  0.776119  0.638037   67.000000\n",
      "1              0.125000  0.055556  0.076923   36.000000\n",
      "2              0.125000  0.058824  0.080000   17.000000\n",
      "accuracy       0.458333  0.458333  0.458333    0.458333\n",
      "macro avg      0.263889  0.296833  0.264987  120.000000\n",
      "weighted avg   0.357639  0.458333  0.390647  120.000000\n",
      "Epoch: 300\n",
      "              precision    recall  f1-score  support\n",
      "0              0.536842  0.761194  0.629630    67.00\n",
      "1              0.117647  0.055556  0.075472    36.00\n",
      "2              0.125000  0.058824  0.080000    17.00\n",
      "accuracy       0.450000  0.450000  0.450000     0.45\n",
      "macro avg      0.259830  0.291858  0.261700   120.00\n",
      "weighted avg   0.352739  0.450000  0.385518   120.00\n",
      "Epoch: 350\n",
      "              precision    recall  f1-score     support\n",
      "0              0.574257  0.865672  0.690476   67.000000\n",
      "1              0.272727  0.083333  0.127660   36.000000\n",
      "2              0.125000  0.058824  0.080000   17.000000\n",
      "accuracy       0.516667  0.516667  0.516667    0.516667\n",
      "macro avg      0.323995  0.335943  0.299379  120.000000\n",
      "weighted avg   0.420154  0.516667  0.435147  120.000000\n",
      "Epoch: 400\n",
      "              precision    recall  f1-score  support\n",
      "0              0.565657  0.835821  0.674699     67.0\n",
      "1              0.230769  0.083333  0.122449     36.0\n",
      "2              0.125000  0.058824  0.080000     17.0\n",
      "accuracy       0.500000  0.500000  0.500000      0.5\n",
      "macro avg      0.307142  0.325993  0.292383    120.0\n",
      "weighted avg   0.402764  0.500000  0.424775    120.0\n",
      "Epoch: 450\n",
      "              precision    recall  f1-score  support\n",
      "0              0.557692  0.865672  0.678363   67.000\n",
      "1              0.363636  0.111111  0.170213   36.000\n",
      "2              0.200000  0.058824  0.090909   17.000\n",
      "accuracy       0.525000  0.525000  0.525000    0.525\n",
      "macro avg      0.373776  0.345202  0.313161  120.000\n",
      "weighted avg   0.448802  0.525000  0.442695  120.000\n",
      "Epoch: 500\n",
      "              precision    recall  f1-score     support\n",
      "0              0.553398  0.850746  0.670588   67.000000\n",
      "1              0.272727  0.083333  0.127660   36.000000\n",
      "2              0.166667  0.058824  0.086957   17.000000\n",
      "accuracy       0.508333  0.508333  0.508333    0.508333\n",
      "macro avg      0.330931  0.330968  0.295068  120.000000\n",
      "weighted avg   0.414410  0.508333  0.425028  120.000000\n",
      "Epoch: 550\n",
      "              precision    recall  f1-score     support\n",
      "0              0.556604  0.880597  0.682081   67.000000\n",
      "1              0.400000  0.111111  0.173913   36.000000\n",
      "2              0.250000  0.058824  0.095238   17.000000\n",
      "accuracy       0.533333  0.533333  0.533333    0.533333\n",
      "macro avg      0.402201  0.350177  0.317077  120.000000\n",
      "weighted avg   0.466187  0.533333  0.446494  120.000000\n",
      "Epoch: 600\n",
      "              precision    recall  f1-score  support\n",
      "0              0.556604  0.880597  0.682081   67.000\n",
      "1              0.333333  0.083333  0.133333   36.000\n",
      "2              0.200000  0.058824  0.090909   17.000\n",
      "accuracy       0.525000  0.525000  0.525000    0.525\n",
      "macro avg      0.363312  0.340918  0.302108  120.000\n",
      "weighted avg   0.439104  0.525000  0.433707  120.000\n",
      "Epoch: 650\n",
      "              precision    recall  f1-score  support\n",
      "0              0.556604  0.880597  0.682081   67.000\n",
      "1              0.333333  0.083333  0.133333   36.000\n",
      "2              0.200000  0.058824  0.090909   17.000\n",
      "accuracy       0.525000  0.525000  0.525000    0.525\n",
      "macro avg      0.363312  0.340918  0.302108  120.000\n",
      "weighted avg   0.439104  0.525000  0.433707  120.000\n",
      "Epoch: 700\n",
      "              precision    recall  f1-score     support\n",
      "0              0.572816  0.880597  0.694118   67.000000\n",
      "1              0.538462  0.194444  0.285714   36.000000\n",
      "2              0.250000  0.058824  0.095238   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.453759  0.377955  0.358357  120.000000\n",
      "weighted avg   0.516777  0.558333  0.486755  120.000000\n",
      "Epoch: 750\n",
      "              precision    recall  f1-score     support\n",
      "0              0.572816  0.880597  0.694118   67.000000\n",
      "1              0.500000  0.194444  0.280000   36.000000\n",
      "2              0.333333  0.058824  0.100000   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.468716  0.377955  0.358039  120.000000\n",
      "weighted avg   0.517044  0.558333  0.485716  120.000000\n",
      "Epoch: 800\n",
      "              precision    recall  f1-score     support\n",
      "0              0.578431  0.880597  0.698225   67.000000\n",
      "1              0.533333  0.222222  0.313725   36.000000\n",
      "2              0.333333  0.058824  0.100000   17.000000\n",
      "accuracy       0.566667  0.566667  0.566667    0.566667\n",
      "macro avg      0.481699  0.387214  0.370650  120.000000\n",
      "weighted avg   0.530180  0.566667  0.498127  120.000000\n",
      "Epoch: 850\n",
      "              precision    recall  f1-score     support\n",
      "0              0.578431  0.880597  0.698225   67.000000\n",
      "1              0.533333  0.222222  0.313725   36.000000\n",
      "2              0.333333  0.058824  0.100000   17.000000\n",
      "accuracy       0.566667  0.566667  0.566667    0.566667\n",
      "macro avg      0.481699  0.387214  0.370650  120.000000\n",
      "weighted avg   0.530180  0.566667  0.498127  120.000000\n",
      "Epoch: 900\n",
      "              precision    recall  f1-score     support\n",
      "0              0.584158  0.880597  0.702381   67.000000\n",
      "1              0.571429  0.222222  0.320000   36.000000\n",
      "2              0.200000  0.058824  0.090909   17.000000\n",
      "accuracy       0.566667  0.566667  0.566667    0.566667\n",
      "macro avg      0.451862  0.387214  0.371097  120.000000\n",
      "weighted avg   0.525917  0.566667  0.501041  120.000000\n",
      "Epoch: 950\n",
      "              precision    recall  f1-score     support\n",
      "0              0.584158  0.880597  0.702381   67.000000\n",
      "1              0.571429  0.222222  0.320000   36.000000\n",
      "2              0.200000  0.058824  0.090909   17.000000\n",
      "accuracy       0.566667  0.566667  0.566667    0.566667\n",
      "macro avg      0.451862  0.387214  0.371097  120.000000\n",
      "weighted avg   0.525917  0.566667  0.501041  120.000000\n",
      "Epoch: 1000\n",
      "              precision    recall  f1-score     support\n",
      "0              0.580000  0.865672  0.694611   67.000000\n",
      "1              0.533333  0.222222  0.313725   36.000000\n",
      "2              0.200000  0.058824  0.090909   17.000000\n",
      "accuracy       0.558333  0.558333  0.558333    0.558333\n",
      "macro avg      0.437778  0.382239  0.366415  120.000000\n",
      "weighted avg   0.512167  0.558333  0.494821  120.000000\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1001\n",
    "    \n",
    "test_min_loss = np.inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.manual_seed(42)\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    gru.train()\n",
    "    for data, target in train_loader:\n",
    "        h = gru.init_hidden(batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output, h = gru(data, h.data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    gru.eval()\n",
    "    for data, target in test_loader:\n",
    "        if data.shape[1] < 48:\n",
    "            continue\n",
    "        h = gru.init_hidden(batch_size)\n",
    "        output, h = gru(data, h.data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()*data.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "#     if(epoch%20 == 0):\n",
    "# #         print(f\"Epoch: {epoch+1:02}\")\n",
    "# #         print(\"\\tTraining Loss: {:.6f} \\Test Loss: {:.6f}\".format(train_loss, test_loss))\n",
    "#     if test_loss <= test_min_loss:\n",
    "# #         print(\"Test loss decreased ({:.6f} --> {:.6f}). Saving model...\".format(test_min_loss, test_loss))\n",
    "# #         torch.save(gru.state_dict(), \"fnnmodel.pt\")\n",
    "#         test_min_loss = test_loss\n",
    "    if(epoch%50 == 0):\n",
    "        torch.save(gru.state_dict(), \"gru/gru_\" + str(epoch) + \"_.pt\")\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        test_loader = torch.utils.data.DataLoader(fnn_test_tensor, batch_size=fnn_test_tensor.__len__())\n",
    "        predictions, actuals = test_accuracy(gru, test_loader, fnn_test_tensor.__len__())\n",
    "        print(pd.DataFrame(classification_report(actuals, predictions, output_dict=True)).T)\n",
    "        test_loader = torch.utils.data.DataLoader(fnn_test_tensor, batch_size=batch_size, sampler=SubsetRandomSampler(test_indices),drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80c9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
