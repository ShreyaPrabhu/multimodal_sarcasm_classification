{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809d57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "desired_frames = 1\n",
    "desired_features = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b3e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"/Users/yoshithaakunuri/Documents/CSCI535/Project/Final/data/scene_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17a8765",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCENE</th>\n",
       "      <th>KEY</th>\n",
       "      <th>SPEAKER</th>\n",
       "      <th>SHOW</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Sarcasm_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_10004</td>\n",
       "      <td>1_10004_u</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_10009</td>\n",
       "      <td>1_10009_u</td>\n",
       "      <td>PENNY</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1001</td>\n",
       "      <td>1_1001_u</td>\n",
       "      <td>RAJ</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1003</td>\n",
       "      <td>1_1003_u</td>\n",
       "      <td>HOWARD</td>\n",
       "      <td>BBT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_10190</td>\n",
       "      <td>1_10190_u</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>BBT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SCENE        KEY  SPEAKER SHOW  Sarcasm Sarcasm_Type\n",
       "0  1_10004  1_10004_u  SHELDON  BBT      0.0         NONE\n",
       "1  1_10009  1_10009_u    PENNY  BBT      0.0         NONE\n",
       "2   1_1001   1_1001_u      RAJ  BBT      0.0         NONE\n",
       "3   1_1003   1_1003_u   HOWARD  BBT      1.0          PRO\n",
       "4  1_10190  1_10190_u  SHELDON  BBT      0.0         NONE"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9904e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = list(pd.unique(labels[\"SCENE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88143cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1202"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cdb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"/Users/yoshithaakunuri/Documents/CSCI535/Project/Final/data/features\"\n",
    "visual_features = {}\n",
    "context_visual_features = {}\n",
    "for i in range(len(scenes)):\n",
    "    # Visual Features\n",
    "    try:\n",
    "        vf = np.load(os.path.join(parent_dir, \"utterances_final\", \"resnet_pool5_\" + scenes[i] + \".npy\"))\n",
    "        #Global average pooling\n",
    "        vf_p = np.apply_over_axes(np.mean, vf, [2, 3]) \n",
    "        vf_p = np.reshape(vf_p, (vf_p.shape[0],2048))\n",
    "        if vf_p.shape[0] != 15:\n",
    "            vf_p = np.pad(vf_p, [(0, 15 - vf_p.shape[0]), (0, 0)], mode='constant')\n",
    "    except:\n",
    "        vf_p = np.zeros((15, 2048))\n",
    "    \n",
    "    # Visual Features - Context\n",
    "    try:\n",
    "        c_vf = np.load(os.path.join(parent_dir, \"context_final\", \"resnet_pool5_\" + scenes[i] + \"_c.npy\"))\n",
    "        #Global average pooling\n",
    "        c_vf_p = np.apply_over_axes(np.mean, c_vf, [2, 3]) \n",
    "        c_vf_p = np.reshape(c_vf_p, (c_vf_p.shape[0],2048))\n",
    "        if c_vf_p.shape[0] != 15:\n",
    "            c_vf_p = np.pad(c_vf_p, [(0, 15 - c_vf_p.shape[0]), (0, 0)], mode='constant')\n",
    "    except:\n",
    "        c_vf_p = np.zeros((15, 2048))\n",
    "    \n",
    "    visual_features[scenes[i]] = vf_p\n",
    "    context_visual_features[scenes[i]] = c_vf_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f80367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(context_video_features, video_features):\n",
    "    model_data = pd.DataFrame(columns=['context_video_feature', 'video_feature','sarcasm','sarcasm_type', 'speaker'])\n",
    "    for index, row in labels.iterrows():\n",
    "#         audio_key = row[\"SCENE\"] + \"_u.wav\"\n",
    "        model_data = model_data.append({'context_video_feature': context_video_features[row[\"SCENE\"]],\n",
    "                                    'video_feature': video_features[row[\"SCENE\"]],\n",
    "                                    'sarcasm' : row[\"Sarcasm\"],\n",
    "                                    'sarcasm_type' : row[\"Sarcasm_Type\"],\n",
    "                                    'speaker' : row[\"SPEAKER\"]},\n",
    "                                  ignore_index=True)\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39e9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(model_data, x_columns, y_column, stratify_column):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        model_data[x_columns],\n",
    "        model_data[y_column],\n",
    "        train_size=0.8, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        shuffle=True,\n",
    "        stratify=model_data[stratify_column])\n",
    "    \n",
    "    print(\"Train: \",X_train.shape, Y_train.shape,\n",
    "      \"Test: \",(X_test.shape, Y_test.shape))\n",
    "    train_data = pd.merge(X_train, Y_train, left_index=True, right_index=True)\n",
    "    test_data = pd.merge(X_test, Y_test, left_index=True, right_index=True)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d753e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTensorDataset(Dataset):\n",
    "    def __init__(self, dataframe, speaker):\n",
    "        self.data = dataframe\n",
    "        self.speaker = speaker\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.speaker:\n",
    "            features = self.data.loc[index, 'padded_video_feature']\n",
    "            ctxt_features = self.data.loc[index, 'padded_context_video_feature']\n",
    "            final_features = np.vstack((ctxt_features, features))\n",
    "            a=np.empty((30,1))\n",
    "            a.fill(self.data.loc[index, 'speaker_encode'])\n",
    "            final_features = np.hstack((final_features, a))\n",
    "            label = self.data.loc[index, 'sarcasm']\n",
    "            return torch.from_numpy(final_features).float(), label\n",
    "        else:\n",
    "            features = self.data.loc[index, 'padded_video_feature']\n",
    "            ctxt_features = self.data.loc[index, 'padded_context_video_feature']\n",
    "            final_features = np.vstack((ctxt_features, features))\n",
    "            label = self.data.loc[index, 'sarcasm']\n",
    "            return torch.from_numpy(final_features).float(), label\n",
    "    \n",
    "    def __getindexlist__(self):\n",
    "        return list(self.data.index.values)\n",
    "    \n",
    "class RNNetSD(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
    "        super(RNNetSD, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True, nonlinearity=\"relu\")\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = Variable(torch.zeros(self.layers, x.size(0), self.hidden_dim))\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = F.softmax(self.fc(out[:, -1, :]))\n",
    "        return out\n",
    "    \n",
    "class RNNetSID(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
    "        super(RNNetSID, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True, nonlinearity=\"relu\")\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = Variable(torch.zeros(self.layers, x.size(0), self.hidden_dim))\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = F.softmax(self.fc(out[:, -1, :]))\n",
    "        return out\n",
    "    \n",
    "def evaluateRNN(rnn, review, size):\n",
    "    output = rnn(review)\n",
    "    return output\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = torch.max(output,dim=1)\n",
    "    return top_i\n",
    "\n",
    "def test_accuracy(rnn, loader, size):\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    for data, target in loader:\n",
    "        output = evaluateRNN(rnn, data, size)\n",
    "        prediction_index = categoryFromOutput(output)\n",
    "        predictions = prediction_index.tolist()\n",
    "        actuals = target.tolist()\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508b145f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_video_feature</th>\n",
       "      <th>video_feature</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sarcasm_type</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_encode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.30626756, 0.66872513, 0.28155187, 0.312841...</td>\n",
       "      <td>[[0.13835047, 0.2704592, 0.44648886, 0.1415337...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.17222668, 0.7350801, 0.30408216, 0.3297085...</td>\n",
       "      <td>[[0.46479157, 0.1813915, 0.22123067, 0.5245148...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>PENNY</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.048892517, 0.23698963, 0.4193368, 0.341263...</td>\n",
       "      <td>[[0.253619, 0.25664786, 0.6646118, 0.4821793, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>RAJ</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.5546928, 0.36914897, 0.8243424, 0.18547274...</td>\n",
       "      <td>[[0.55624646, 0.16990338, 0.62457716, 0.209021...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PRO</td>\n",
       "      <td>HOWARD</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.73847526, 1.0405933, 0.11508263, 0.9313859...</td>\n",
       "      <td>[[0.6140023, 0.4846397, 0.79425097, 0.13518682...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               context_video_feature  \\\n",
       "0  [[0.30626756, 0.66872513, 0.28155187, 0.312841...   \n",
       "1  [[0.17222668, 0.7350801, 0.30408216, 0.3297085...   \n",
       "2  [[0.048892517, 0.23698963, 0.4193368, 0.341263...   \n",
       "3  [[0.5546928, 0.36914897, 0.8243424, 0.18547274...   \n",
       "4  [[0.73847526, 1.0405933, 0.11508263, 0.9313859...   \n",
       "\n",
       "                                       video_feature  sarcasm sarcasm_type  \\\n",
       "0  [[0.13835047, 0.2704592, 0.44648886, 0.1415337...      0.0         NONE   \n",
       "1  [[0.46479157, 0.1813915, 0.22123067, 0.5245148...      0.0         NONE   \n",
       "2  [[0.253619, 0.25664786, 0.6646118, 0.4821793, ...      0.0         NONE   \n",
       "3  [[0.55624646, 0.16990338, 0.62457716, 0.209021...      1.0          PRO   \n",
       "4  [[0.6140023, 0.4846397, 0.79425097, 0.13518682...      0.0         NONE   \n",
       "\n",
       "   speaker  speaker_encode  \n",
       "0  SHELDON              25  \n",
       "1    PENNY              15  \n",
       "2      RAJ              21  \n",
       "3   HOWARD               7  \n",
       "4  SHELDON              25  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_data = get_model_data(context_visual_features, visual_features)\n",
    "# Label Encode Speaker\n",
    "le = preprocessing.LabelEncoder()\n",
    "model_data['speaker_encode'] = le.fit_transform(model_data['speaker'])\n",
    "model_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2508c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (961, 3) (961,) Test:  ((241, 3), (241,))\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = get_train_test_split(model_data, ['context_video_feature', 'video_feature', 'speaker_encode'], 'sarcasm', 'sarcasm_type')\n",
    "rnn_train = train_data.copy()\n",
    "rnn_test = test_data.copy()\n",
    "rnn_train.reset_index(drop=True, inplace = True)\n",
    "rnn_test.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79f6e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_length = 15\n",
    "desired_length_ctxt = 15\n",
    "\n",
    "rnn_train['padded_video_feature'] = rnn_train.loc[:, 'video_feature']\n",
    "rnn_train['padded_context_video_feature'] = rnn_train.loc[:, 'context_video_feature']\n",
    "\n",
    "rnn_test['padded_video_feature'] = rnn_test.loc[:, 'video_feature']\n",
    "rnn_test['padded_context_video_feature'] = rnn_test.loc[:, 'video_feature']\n",
    "\n",
    "rnn_train[\"sarcasm\"] = rnn_train[\"sarcasm\"].astype('int').to_numpy()\n",
    "rnn_test[\"sarcasm\"] = rnn_test[\"sarcasm\"].astype('int').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eee7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM_sid = 2048\n",
    "EMBEDDING_DIM_sd = 2049\n",
    "HIDDEN_DIM = 20\n",
    "OUTPUT_DIM = 2\n",
    "layers = 2\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e6b92",
   "metadata": {},
   "source": [
    "### Speaker Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afd381d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_train_tensor = RNNTensorDataset(rnn_train[['padded_context_video_feature', 'padded_video_feature', 'sarcasm']], False)\n",
    "rnn_test_tensor = RNNTensorDataset(rnn_test[['padded_context_video_feature', 'padded_video_feature', 'sarcasm']], False)\n",
    "\n",
    "num_of_workers = 0\n",
    "batch_size = 31\n",
    "valid_size = 0.1\n",
    "\n",
    "train_indices = list(range(len(rnn_train_tensor)))\n",
    "np.random.shuffle(train_indices)\n",
    "\n",
    "test_indices = list(range(len(rnn_test_tensor)))\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    rnn_train_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(train_indices)\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    rnn_test_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(test_indices)\n",
    ")\n",
    "\n",
    "test_loader_epoch = torch.utils.data.DataLoader(\n",
    "    rnn_test_tensor, batch_size=rnn_test_tensor.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc665954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNetSID(\n",
      "  (rnn): RNN(2048, 20, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn = RNNetSID(EMBEDDING_DIM_sid, HIDDEN_DIM, OUTPUT_DIM, layers)\n",
    "print(rnn)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c349baf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5311    0.9250    0.6748       120\n",
      "           1     0.7188    0.1901    0.3007       121\n",
      "\n",
      "    accuracy                         0.5560       241\n",
      "   macro avg     0.6249    0.5575    0.4877       241\n",
      "weighted avg     0.6253    0.5560    0.4869       241\n",
      "\n",
      "Epoch: 20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6121    0.5917    0.6017       120\n",
      "           1     0.6080    0.6281    0.6179       121\n",
      "\n",
      "    accuracy                         0.6100       241\n",
      "   macro avg     0.6100    0.6099    0.6098       241\n",
      "weighted avg     0.6100    0.6100    0.6098       241\n",
      "\n",
      "Epoch: 40\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4979    1.0000    0.6648       120\n",
      "           1     0.0000    0.0000    0.0000       121\n",
      "\n",
      "    accuracy                         0.4979       241\n",
      "   macro avg     0.2490    0.5000    0.3324       241\n",
      "weighted avg     0.2479    0.4979    0.3310       241\n",
      "\n",
      "Epoch: 60\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4979    1.0000    0.6648       120\n",
      "           1     0.0000    0.0000    0.0000       121\n",
      "\n",
      "    accuracy                         0.4979       241\n",
      "   macro avg     0.2490    0.5000    0.3324       241\n",
      "weighted avg     0.2479    0.4979    0.3310       241\n",
      "\n",
      "Epoch: 80\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4979    1.0000    0.6648       120\n",
      "           1     0.0000    0.0000    0.0000       121\n",
      "\n",
      "    accuracy                         0.4979       241\n",
      "   macro avg     0.2490    0.5000    0.3324       241\n",
      "weighted avg     0.2479    0.4979    0.3310       241\n",
      "\n",
      "Epoch: 100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4979    1.0000    0.6648       120\n",
      "           1     0.0000    0.0000    0.0000       121\n",
      "\n",
      "    accuracy                         0.4979       241\n",
      "   macro avg     0.2490    0.5000    0.3324       241\n",
      "weighted avg     0.2479    0.4979    0.3310       241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 101\n",
    "    \n",
    "test_min_loss = np.inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.manual_seed(42)\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    rnn.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    rnn.eval()\n",
    "    for data, target in test_loader:\n",
    "        if data.shape[1] < 31:\n",
    "            continue\n",
    "        output = rnn(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()*data.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    if(epoch%20 == 0):\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        predictions, actuals = test_accuracy(rnn, test_loader_epoch, rnn_test_tensor.__len__())\n",
    "        print(classification_report(actuals, predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879089b1",
   "metadata": {},
   "source": [
    "### Speaker Dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f972827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_train_tensor = RNNTensorDataset(rnn_train[['padded_context_video_feature', 'padded_video_feature', 'speaker_encode','sarcasm']], True)\n",
    "rnn_test_tensor = RNNTensorDataset(rnn_test[['padded_context_video_feature', 'padded_video_feature', 'speaker_encode', 'sarcasm']], True)\n",
    "\n",
    "num_of_workers = 0\n",
    "batch_size = 31\n",
    "valid_size = 0.1\n",
    "\n",
    "train_indices = list(range(len(rnn_train_tensor)))\n",
    "np.random.shuffle(train_indices)\n",
    "\n",
    "test_indices = list(range(len(rnn_test_tensor)))\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    rnn_train_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(train_indices)\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    rnn_test_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(test_indices)\n",
    ")\n",
    "\n",
    "test_loader_epoch = torch.utils.data.DataLoader(\n",
    "    rnn_test_tensor, batch_size=rnn_test_tensor.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c36ed74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNetSD(\n",
      "  (rnn): RNN(2049, 20, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn = RNNetSD(EMBEDDING_DIM_sd, HIDDEN_DIM, OUTPUT_DIM, layers)\n",
    "print(rnn)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d915c1f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6127    0.7250    0.6641       120\n",
      "           1     0.6667    0.5455    0.6000       121\n",
      "\n",
      "    accuracy                         0.6349       241\n",
      "   macro avg     0.6397    0.6352    0.6321       241\n",
      "weighted avg     0.6398    0.6349    0.6319       241\n",
      "\n",
      "Epoch: 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.5833    0.6364       120\n",
      "           1     0.6454    0.7521    0.6947       121\n",
      "\n",
      "    accuracy                         0.6680       241\n",
      "   macro avg     0.6727    0.6677    0.6655       241\n",
      "weighted avg     0.6726    0.6680    0.6656       241\n",
      "\n",
      "Epoch: 20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7111    0.5333    0.6095       120\n",
      "           1     0.6291    0.7851    0.6985       121\n",
      "\n",
      "    accuracy                         0.6598       241\n",
      "   macro avg     0.6701    0.6592    0.6540       241\n",
      "weighted avg     0.6700    0.6598    0.6542       241\n",
      "\n",
      "Epoch: 30\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7041    0.5750    0.6330       120\n",
      "           1     0.6434    0.7603    0.6970       121\n",
      "\n",
      "    accuracy                         0.6680       241\n",
      "   macro avg     0.6737    0.6677    0.6650       241\n",
      "weighted avg     0.6736    0.6680    0.6651       241\n",
      "\n",
      "Epoch: 40\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    0.2500    0.3750       120\n",
      "           1     0.5522    0.9174    0.6894       121\n",
      "\n",
      "    accuracy                         0.5851       241\n",
      "   macro avg     0.6511    0.5837    0.5322       241\n",
      "weighted avg     0.6507    0.5851    0.5329       241\n",
      "\n",
      "Epoch: 50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7041    0.5750    0.6330       120\n",
      "           1     0.6434    0.7603    0.6970       121\n",
      "\n",
      "    accuracy                         0.6680       241\n",
      "   macro avg     0.6737    0.6677    0.6650       241\n",
      "weighted avg     0.6736    0.6680    0.6651       241\n",
      "\n",
      "Epoch: 60\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6889    0.5167    0.5905       120\n",
      "           1     0.6159    0.7686    0.6838       121\n",
      "\n",
      "    accuracy                         0.6432       241\n",
      "   macro avg     0.6524    0.6426    0.6371       241\n",
      "weighted avg     0.6522    0.6432    0.6373       241\n",
      "\n",
      "Epoch: 70\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7931    0.1917    0.3087       120\n",
      "           1     0.5425    0.9504    0.6907       121\n",
      "\n",
      "    accuracy                         0.5726       241\n",
      "   macro avg     0.6678    0.5710    0.4997       241\n",
      "weighted avg     0.6673    0.5726    0.5005       241\n",
      "\n",
      "Epoch: 80\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.2000    0.3200       120\n",
      "           1     0.5450    0.9504    0.6928       121\n",
      "\n",
      "    accuracy                         0.5768       241\n",
      "   macro avg     0.6725    0.5752    0.5064       241\n",
      "weighted avg     0.6720    0.5768    0.5072       241\n",
      "\n",
      "Epoch: 90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7619    0.4000    0.5246       120\n",
      "           1     0.5955    0.8760    0.7090       121\n",
      "\n",
      "    accuracy                         0.6390       241\n",
      "   macro avg     0.6787    0.6380    0.6168       241\n",
      "weighted avg     0.6784    0.6390    0.6172       241\n",
      "\n",
      "Epoch: 100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7794    0.4417    0.5638       120\n",
      "           1     0.6127    0.8760    0.7211       121\n",
      "\n",
      "    accuracy                         0.6598       241\n",
      "   macro avg     0.6961    0.6588    0.6425       241\n",
      "weighted avg     0.6957    0.6598    0.6428       241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 101\n",
    "    \n",
    "test_min_loss = np.inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.manual_seed(42)\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    rnn.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    rnn.eval()\n",
    "    for data, target in test_loader:\n",
    "        if data.shape[1] < 31:\n",
    "            continue\n",
    "        output = rnn(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()*data.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    if(epoch%10 == 0):\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        predictions, actuals = test_accuracy(rnn, test_loader_epoch, rnn_test_tensor.__len__())\n",
    "        print(classification_report(actuals, predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19295037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93150d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26016c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
